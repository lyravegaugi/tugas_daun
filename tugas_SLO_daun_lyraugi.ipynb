{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tugas_SLO_daun_lyraugi.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOAg6bpZuqbuMuR8ZYidL9I",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lyravegaugi/tugas_daun/blob/main/tugas_SLO_daun_lyraugi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YubG4-HxgthM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e5fb188-7011-4986-8576-d4988832e2ff"
      },
      "source": [
        "print('Lyra Vega Ugi')\n",
        "print('2101201021')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Lyra Vega Ugi\n",
            "2101201021\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzrtFHnWgwEO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a83728c-54b0-49ee-bfe2-0af36e9c075c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M87cPbT1gzES",
        "outputId": "8c763163-5f31-4910-87db-23453f101c3b"
      },
      "source": [
        "cd /content/drive/MyDrive/"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "02gZfaQDhUsS",
        "outputId": "f8ea526e-170f-4838-e580-33ecb5c683ea"
      },
      "source": [
        "ls"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " best_weight_scene_mobileNet.h5  'dza-hsis-coj - Oct 27, 2020 (1).gjam'\n",
            "\u001b[0m\u001b[01;34m'Colab Notebooks'\u001b[0m/               'dza-hsis-coj - Oct 27, 2020.gjam'\n",
            " \u001b[01;34mdata_daun\u001b[0m/                      'Getting started.pdf'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MbqBRSxlhXAc"
      },
      "source": [
        "# import the necessary packages\n",
        "from keras.models import Sequential\n",
        "from keras.layers.convolutional import Conv2D\n",
        "from keras.layers.convolutional import MaxPooling2D\n",
        "from keras.layers.core import Activation\n",
        "from keras.layers.core import Flatten\n",
        "from keras.layers.core import Dense\n",
        "from keras.optimizers import Adam, SGD, RMSprop\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from PIL import Image\n",
        "from imutils import paths\n",
        "import numpy as np\n",
        "import os"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mE6dBzG1hZQn",
        "outputId": "74150979-2133-4e1b-eac7-6dd63180af6a"
      },
      "source": [
        "# grab all image paths in the input dataset directory, then initialize\n",
        "# our list of images and corresponding class labels\n",
        "print(\"[INFO] loading images...\")\n",
        "imagePaths = paths.list_images(\"data_daun\")\n",
        "data = []\n",
        "labels = []"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] loading images...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ob48lRhGhbMg"
      },
      "source": [
        "# loop over our input images\n",
        "for imagePath in imagePaths:\n",
        "\t# load the input image from disk, resize it to 32x32 pixels, scale\n",
        "\t# the pixel intensities to the range [0, 1], and then update our\n",
        "\t# images list\n",
        "\timage = Image.open(imagePath)\n",
        "\timage = np.array(image.resize((224, 224))) / 255.0 #normalisasi\n",
        "\tdata.append(image)\n",
        "\n",
        "\t# extract the class label from the file path and update the\n",
        "\t# labels list\n",
        "\tlabel = imagePath.split(os.path.sep)[-2]\n",
        "\tlabels.append(label)"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "149vtkIRhdDe",
        "outputId": "567a4038-c9e4-434e-ce41-31bb76909bd3"
      },
      "source": [
        "print(labels)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['GMB_10', 'GMB_10', 'GMB_10', 'GMB_10', 'GMB_10', 'GMB_10', 'GMB_10', 'GMB_10', 'GMB_10', 'GMB_10', 'GMB_10', 'GMB_10', 'GMB_10', 'GMB_10', 'GMB_10', 'GMB_10', 'GMB_10', 'GMB_10', 'GMB_10', 'GMB_10', 'GMB_10', 'GMB_10', 'GMB_10', 'GMB_10', 'GMB_10', 'GMB_10', 'GMB_10', 'GMB_10', 'GMB_10', 'GMB_10', 'GMB_10', 'GMB_10', 'GMB_10', 'GMB_10', 'GMB_10', 'GMB_10', 'GMB_10', 'GMB_10', 'GMB_10', 'GMB_10', 'GMB_10', 'GMB_10', 'GMB_10', 'GMB_10', 'GMB_10', 'GMB_10', 'GMB_10', 'GMB_10', 'GMB_10', 'GMB_10', 'GMB_10', 'GMB_10', 'GMB_10', 'GMB_10', 'GMB_10', 'GMB_10', 'GMB_10', 'GMB_10', 'GMB_10', 'GMB_10', 'GMB_10', 'GMB_10', 'GMB_10', 'GMB_10', 'GMB_10', 'GMB_10', 'GMB_10', 'GMB_10', 'GMB_10', 'GMB_10', 'GMB_10', 'GMB_10', 'GMB_10', 'GMB_10', 'GMB_10', 'GMB_10', 'GMB_10', 'GMB_10', 'GMB_10', 'GMB_10', 'GMB_10', 'GMB_10', 'GMB_10', 'GMB_10', 'GMB_10', 'GMB_10', 'GMB_10', 'GMB_10', 'GMB_10', 'GMB_10', 'GMB_10', 'GMB_10', 'GMB_10', 'GMB_10', 'GMB_10', 'GMB_10', 'GMB_10', 'GMB_10', 'GMB_10', 'GMB_10', 'GMB_10', 'GMB_10', 'GMB_10', 'GMB_10', 'GMB_10', 'GMB_10', 'GMB_10', 'GMB_10', 'GMB_10', 'GMB_10', 'GMB_10', 'GMB_10', 'GMB_10', 'GMB_10', 'GMB_04', 'GMB_04', 'GMB_04', 'GMB_04', 'GMB_04', 'GMB_04', 'GMB_04', 'GMB_04', 'GMB_04', 'GMB_04', 'GMB_04', 'GMB_04', 'GMB_04', 'GMB_04', 'GMB_04', 'GMB_04', 'GMB_04', 'GMB_04', 'GMB_04', 'GMB_04', 'GMB_04', 'GMB_04', 'GMB_04', 'GMB_04', 'GMB_04', 'GMB_04', 'GMB_04', 'GMB_04', 'GMB_04', 'GMB_04', 'GMB_04', 'GMB_04', 'GMB_04', 'GMB_04', 'GMB_04', 'GMB_04', 'GMB_04', 'GMB_04', 'GMB_04', 'GMB_04', 'GMB_04', 'GMB_04', 'GMB_04', 'GMB_04', 'GMB_04', 'GMB_04', 'GMB_04', 'GMB_04', 'GMB_04', 'GMB_04', 'GMB_04', 'GMB_04', 'GMB_04', 'GMB_04', 'GMB_04', 'GMB_04', 'GMB_04', 'GMB_04', 'GMB_04', 'GMB_04', 'GMB_04', 'GMB_04', 'GMB_04', 'GMB_04', 'GMB_04', 'GMB_04', 'GMB_04', 'GMB_04', 'GMB_04', 'GMB_04', 'GMB_04', 'GMB_04', 'GMB_04', 'GMB_04', 'GMB_04', 'GMB_04', 'GMB_04', 'GMB_04', 'GMB_04', 'GMB_04', 'GMB_04', 'GMB_04', 'GMB_04', 'GMB_04', 'GMB_04', 'GMB_04', 'GMB_04', 'GMB_04', 'GMB_04', 'GMB_04', 'GMB_04', 'GMB_04', 'GMB_04', 'GMB_04', 'GMB_04', 'GMB_04', 'GMB_04', 'GMB_04', 'GMB_04', 'GMB_04', 'GMB_04', 'GMB_04', 'GMB_04', 'GMB_04', 'GMB_04', 'GMB_04', 'GMB_04', 'GMB_04', 'GMB_04', 'GMB_04', 'GMB_04', 'GMB_04', 'GMB_04', 'GMB_04', 'GMB_04', 'GMB_04', 'GMB_04', 'GMB_04', 'GMB_04', 'GMB_04', 'GMB_04', 'GMB_04', 'GMB_02', 'GMB_02', 'GMB_02', 'GMB_02', 'GMB_02', 'GMB_02', 'GMB_02', 'GMB_02', 'GMB_02', 'GMB_02', 'GMB_02', 'GMB_02', 'GMB_02', 'GMB_02', 'GMB_02', 'GMB_02', 'GMB_02', 'GMB_02', 'GMB_02', 'GMB_02', 'GMB_02', 'GMB_02', 'GMB_02', 'GMB_02', 'GMB_02', 'GMB_02', 'GMB_02', 'GMB_02', 'GMB_02', 'GMB_02', 'GMB_02', 'GMB_02', 'GMB_02', 'GMB_02', 'GMB_02', 'GMB_02', 'GMB_02', 'GMB_02', 'GMB_02', 'GMB_02', 'GMB_02', 'GMB_02', 'GMB_02', 'GMB_02', 'GMB_02', 'GMB_02', 'GMB_02', 'GMB_02', 'GMB_02', 'GMB_02', 'GMB_02', 'GMB_02', 'GMB_02', 'GMB_02', 'GMB_02', 'GMB_02', 'GMB_02', 'GMB_02', 'GMB_02', 'GMB_02', 'GMB_02', 'GMB_02', 'GMB_02', 'GMB_02', 'GMB_02', 'GMB_02', 'GMB_02', 'GMB_02', 'GMB_02', 'GMB_02', 'GMB_02', 'GMB_02', 'GMB_02', 'GMB_02', 'GMB_02', 'GMB_02', 'GMB_02', 'GMB_02', 'GMB_02', 'GMB_02', 'GMB_02', 'GMB_02', 'GMB_02', 'GMB_02', 'GMB_02', 'GMB_02', 'GMB_02', 'GMB_02', 'GMB_02', 'GMB_02', 'GMB_02', 'GMB_02', 'GMB_02', 'GMB_02', 'GMB_02', 'GMB_02', 'GMB_02', 'GMB_02', 'GMB_02', 'GMB_02', 'GMB_02', 'GMB_02', 'GMB_02', 'GMB_02', 'GMB_02', 'GMB_02', 'GMB_02', 'GMB_02', 'GMB_02', 'GMB_02', 'GMB_02', 'GMB_02', 'GMB_02', 'GMB_08', 'GMB_08', 'GMB_08', 'GMB_08', 'GMB_08', 'GMB_08', 'GMB_08', 'GMB_08', 'GMB_08', 'GMB_08', 'GMB_08', 'GMB_08', 'GMB_08', 'GMB_08', 'GMB_08', 'GMB_08', 'GMB_08', 'GMB_08', 'GMB_08', 'GMB_08', 'GMB_08', 'GMB_08', 'GMB_08', 'GMB_08', 'GMB_08', 'GMB_08', 'GMB_08', 'GMB_08', 'GMB_08', 'GMB_08', 'GMB_08', 'GMB_08', 'GMB_08', 'GMB_08', 'GMB_08', 'GMB_08', 'GMB_08', 'GMB_08', 'GMB_08', 'GMB_08', 'GMB_08', 'GMB_08', 'GMB_08', 'GMB_08', 'GMB_08', 'GMB_08', 'GMB_08', 'GMB_08', 'GMB_08', 'GMB_08', 'GMB_08', 'GMB_08', 'GMB_08', 'GMB_08', 'GMB_08', 'GMB_08', 'GMB_08', 'GMB_08', 'GMB_08', 'GMB_08', 'GMB_08', 'GMB_08', 'GMB_08', 'GMB_08', 'GMB_08', 'GMB_08', 'GMB_08', 'GMB_08', 'GMB_08', 'GMB_08', 'GMB_08', 'GMB_08', 'GMB_08', 'GMB_08', 'GMB_08', 'GMB_08', 'GMB_08', 'GMB_08', 'GMB_08', 'GMB_08', 'GMB_08', 'GMB_08', 'GMB_08', 'GMB_08', 'GMB_08', 'GMB_08', 'GMB_08', 'GMB_08', 'GMB_08', 'GMB_08', 'GMB_08', 'GMB_08', 'GMB_08', 'GMB_08', 'GMB_08', 'GMB_08', 'GMB_08', 'GMB_08', 'GMB_08', 'GMB_08', 'GMB_08', 'GMB_08', 'GMB_08', 'GMB_08', 'GMB_08', 'GMB_08', 'GMB_08', 'GMB_08', 'GMB_08', 'GMB_08', 'GMB_08', 'GMB_08', 'GMB_08', 'GMB_08', 'GMB_08', 'GMB_08', 'GMB_08', 'GMB_07', 'GMB_07', 'GMB_07', 'GMB_07', 'GMB_07', 'GMB_07', 'GMB_07', 'GMB_07', 'GMB_07', 'GMB_07', 'GMB_07', 'GMB_07', 'GMB_07', 'GMB_07', 'GMB_07', 'GMB_07', 'GMB_07', 'GMB_07', 'GMB_07', 'GMB_07', 'GMB_07', 'GMB_07', 'GMB_07', 'GMB_07', 'GMB_07', 'GMB_07', 'GMB_07', 'GMB_07', 'GMB_07', 'GMB_07', 'GMB_07', 'GMB_07', 'GMB_07', 'GMB_07', 'GMB_07', 'GMB_07', 'GMB_07', 'GMB_07', 'GMB_07', 'GMB_07', 'GMB_07', 'GMB_07', 'GMB_07', 'GMB_07', 'GMB_07', 'GMB_07', 'GMB_07', 'GMB_07', 'GMB_07', 'GMB_07', 'GMB_07', 'GMB_07', 'GMB_07', 'GMB_07', 'GMB_07', 'GMB_07', 'GMB_07', 'GMB_07', 'GMB_07', 'GMB_07', 'GMB_07', 'GMB_07', 'GMB_07', 'GMB_07', 'GMB_07', 'GMB_07', 'GMB_07', 'GMB_07', 'GMB_07', 'GMB_07', 'GMB_07', 'GMB_07', 'GMB_07', 'GMB_07', 'GMB_07', 'GMB_07', 'GMB_07', 'GMB_07', 'GMB_07', 'GMB_07', 'GMB_07', 'GMB_07', 'GMB_07', 'GMB_07', 'GMB_07', 'GMB_07', 'GMB_07', 'GMB_07', 'GMB_07', 'GMB_07', 'GMB_07', 'GMB_07', 'GMB_07', 'GMB_07', 'GMB_07', 'GMB_07', 'GMB_07', 'GMB_07', 'GMB_07', 'GMB_07', 'GMB_07', 'GMB_07', 'GMB_07', 'GMB_07', 'GMB_07', 'GMB_07', 'GMB_07', 'GMB_07', 'GMB_07', 'GMB_07', 'GMB_07', 'GMB_07', 'GMB_07', 'GMB_07', 'GMB_07', 'GMB_07', 'GMB_07', 'GMB_07', 'GMB_07', 'GMB_07', 'GMB_07']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1WB8u7EzhhHI"
      },
      "source": [
        "# encode the labels, converting them from strings to integers\n",
        "lb = LabelBinarizer()\n",
        "labels = lb.fit_transform(labels)"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B4tfolnshjAa",
        "outputId": "0012e445-e6fb-4d67-dace-fc1af016f9ab"
      },
      "source": [
        "print(labels)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0 0 0 0 1]\n",
            " [0 0 0 0 1]\n",
            " [0 0 0 0 1]\n",
            " ...\n",
            " [0 0 1 0 0]\n",
            " [0 0 1 0 0]\n",
            " [0 0 1 0 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6jXvI7Pqhk3B",
        "outputId": "409ecac5-814e-4b3b-bed3-9692551b627a"
      },
      "source": [
        "# perform a training and testing split, using 75% of the data for\n",
        "# training and 25% for evaluation\n",
        "(trainX, testX, trainY, testY) = train_test_split(np.array(data),\tnp.array(labels), test_size=0.25, random_state=2040, shuffle=True)\n",
        "print(trainX.shape)\n",
        "print(testX.shape)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(440, 224, 224, 3)\n",
            "(147, 224, 224, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HaKLU7dLhnH1"
      },
      "source": [
        "from keras.applications.mobilenet import MobileNet\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from keras.models import Model\n",
        "from keras.layers import GlobalAveragePooling2D, BatchNormalization"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PW9VreWThpUY",
        "outputId": "514a73d2-c52e-4dea-b06b-a7ee74acdb86"
      },
      "source": [
        "from keras.layers import Dropout\n",
        "base_model = MobileNet(weights='imagenet', include_top=False)\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dropout(0.25)(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Dense(512, activation='relu')(x)\n",
        "out = Dense(5, activation='softmax')(x)\n",
        "\n",
        "model = Model(inputs=base_model.input, outputs=out)\n",
        "model.summary()"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
            "Model: \"functional_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_3 (InputLayer)         [(None, None, None, 3)]   0         \n",
            "_________________________________________________________________\n",
            "conv1_pad (ZeroPadding2D)    (None, None, None, 3)     0         \n",
            "_________________________________________________________________\n",
            "conv1 (Conv2D)               (None, None, None, 32)    864       \n",
            "_________________________________________________________________\n",
            "conv1_bn (BatchNormalization (None, None, None, 32)    128       \n",
            "_________________________________________________________________\n",
            "conv1_relu (ReLU)            (None, None, None, 32)    0         \n",
            "_________________________________________________________________\n",
            "conv_dw_1 (DepthwiseConv2D)  (None, None, None, 32)    288       \n",
            "_________________________________________________________________\n",
            "conv_dw_1_bn (BatchNormaliza (None, None, None, 32)    128       \n",
            "_________________________________________________________________\n",
            "conv_dw_1_relu (ReLU)        (None, None, None, 32)    0         \n",
            "_________________________________________________________________\n",
            "conv_pw_1 (Conv2D)           (None, None, None, 64)    2048      \n",
            "_________________________________________________________________\n",
            "conv_pw_1_bn (BatchNormaliza (None, None, None, 64)    256       \n",
            "_________________________________________________________________\n",
            "conv_pw_1_relu (ReLU)        (None, None, None, 64)    0         \n",
            "_________________________________________________________________\n",
            "conv_pad_2 (ZeroPadding2D)   (None, None, None, 64)    0         \n",
            "_________________________________________________________________\n",
            "conv_dw_2 (DepthwiseConv2D)  (None, None, None, 64)    576       \n",
            "_________________________________________________________________\n",
            "conv_dw_2_bn (BatchNormaliza (None, None, None, 64)    256       \n",
            "_________________________________________________________________\n",
            "conv_dw_2_relu (ReLU)        (None, None, None, 64)    0         \n",
            "_________________________________________________________________\n",
            "conv_pw_2 (Conv2D)           (None, None, None, 128)   8192      \n",
            "_________________________________________________________________\n",
            "conv_pw_2_bn (BatchNormaliza (None, None, None, 128)   512       \n",
            "_________________________________________________________________\n",
            "conv_pw_2_relu (ReLU)        (None, None, None, 128)   0         \n",
            "_________________________________________________________________\n",
            "conv_dw_3 (DepthwiseConv2D)  (None, None, None, 128)   1152      \n",
            "_________________________________________________________________\n",
            "conv_dw_3_bn (BatchNormaliza (None, None, None, 128)   512       \n",
            "_________________________________________________________________\n",
            "conv_dw_3_relu (ReLU)        (None, None, None, 128)   0         \n",
            "_________________________________________________________________\n",
            "conv_pw_3 (Conv2D)           (None, None, None, 128)   16384     \n",
            "_________________________________________________________________\n",
            "conv_pw_3_bn (BatchNormaliza (None, None, None, 128)   512       \n",
            "_________________________________________________________________\n",
            "conv_pw_3_relu (ReLU)        (None, None, None, 128)   0         \n",
            "_________________________________________________________________\n",
            "conv_pad_4 (ZeroPadding2D)   (None, None, None, 128)   0         \n",
            "_________________________________________________________________\n",
            "conv_dw_4 (DepthwiseConv2D)  (None, None, None, 128)   1152      \n",
            "_________________________________________________________________\n",
            "conv_dw_4_bn (BatchNormaliza (None, None, None, 128)   512       \n",
            "_________________________________________________________________\n",
            "conv_dw_4_relu (ReLU)        (None, None, None, 128)   0         \n",
            "_________________________________________________________________\n",
            "conv_pw_4 (Conv2D)           (None, None, None, 256)   32768     \n",
            "_________________________________________________________________\n",
            "conv_pw_4_bn (BatchNormaliza (None, None, None, 256)   1024      \n",
            "_________________________________________________________________\n",
            "conv_pw_4_relu (ReLU)        (None, None, None, 256)   0         \n",
            "_________________________________________________________________\n",
            "conv_dw_5 (DepthwiseConv2D)  (None, None, None, 256)   2304      \n",
            "_________________________________________________________________\n",
            "conv_dw_5_bn (BatchNormaliza (None, None, None, 256)   1024      \n",
            "_________________________________________________________________\n",
            "conv_dw_5_relu (ReLU)        (None, None, None, 256)   0         \n",
            "_________________________________________________________________\n",
            "conv_pw_5 (Conv2D)           (None, None, None, 256)   65536     \n",
            "_________________________________________________________________\n",
            "conv_pw_5_bn (BatchNormaliza (None, None, None, 256)   1024      \n",
            "_________________________________________________________________\n",
            "conv_pw_5_relu (ReLU)        (None, None, None, 256)   0         \n",
            "_________________________________________________________________\n",
            "conv_pad_6 (ZeroPadding2D)   (None, None, None, 256)   0         \n",
            "_________________________________________________________________\n",
            "conv_dw_6 (DepthwiseConv2D)  (None, None, None, 256)   2304      \n",
            "_________________________________________________________________\n",
            "conv_dw_6_bn (BatchNormaliza (None, None, None, 256)   1024      \n",
            "_________________________________________________________________\n",
            "conv_dw_6_relu (ReLU)        (None, None, None, 256)   0         \n",
            "_________________________________________________________________\n",
            "conv_pw_6 (Conv2D)           (None, None, None, 512)   131072    \n",
            "_________________________________________________________________\n",
            "conv_pw_6_bn (BatchNormaliza (None, None, None, 512)   2048      \n",
            "_________________________________________________________________\n",
            "conv_pw_6_relu (ReLU)        (None, None, None, 512)   0         \n",
            "_________________________________________________________________\n",
            "conv_dw_7 (DepthwiseConv2D)  (None, None, None, 512)   4608      \n",
            "_________________________________________________________________\n",
            "conv_dw_7_bn (BatchNormaliza (None, None, None, 512)   2048      \n",
            "_________________________________________________________________\n",
            "conv_dw_7_relu (ReLU)        (None, None, None, 512)   0         \n",
            "_________________________________________________________________\n",
            "conv_pw_7 (Conv2D)           (None, None, None, 512)   262144    \n",
            "_________________________________________________________________\n",
            "conv_pw_7_bn (BatchNormaliza (None, None, None, 512)   2048      \n",
            "_________________________________________________________________\n",
            "conv_pw_7_relu (ReLU)        (None, None, None, 512)   0         \n",
            "_________________________________________________________________\n",
            "conv_dw_8 (DepthwiseConv2D)  (None, None, None, 512)   4608      \n",
            "_________________________________________________________________\n",
            "conv_dw_8_bn (BatchNormaliza (None, None, None, 512)   2048      \n",
            "_________________________________________________________________\n",
            "conv_dw_8_relu (ReLU)        (None, None, None, 512)   0         \n",
            "_________________________________________________________________\n",
            "conv_pw_8 (Conv2D)           (None, None, None, 512)   262144    \n",
            "_________________________________________________________________\n",
            "conv_pw_8_bn (BatchNormaliza (None, None, None, 512)   2048      \n",
            "_________________________________________________________________\n",
            "conv_pw_8_relu (ReLU)        (None, None, None, 512)   0         \n",
            "_________________________________________________________________\n",
            "conv_dw_9 (DepthwiseConv2D)  (None, None, None, 512)   4608      \n",
            "_________________________________________________________________\n",
            "conv_dw_9_bn (BatchNormaliza (None, None, None, 512)   2048      \n",
            "_________________________________________________________________\n",
            "conv_dw_9_relu (ReLU)        (None, None, None, 512)   0         \n",
            "_________________________________________________________________\n",
            "conv_pw_9 (Conv2D)           (None, None, None, 512)   262144    \n",
            "_________________________________________________________________\n",
            "conv_pw_9_bn (BatchNormaliza (None, None, None, 512)   2048      \n",
            "_________________________________________________________________\n",
            "conv_pw_9_relu (ReLU)        (None, None, None, 512)   0         \n",
            "_________________________________________________________________\n",
            "conv_dw_10 (DepthwiseConv2D) (None, None, None, 512)   4608      \n",
            "_________________________________________________________________\n",
            "conv_dw_10_bn (BatchNormaliz (None, None, None, 512)   2048      \n",
            "_________________________________________________________________\n",
            "conv_dw_10_relu (ReLU)       (None, None, None, 512)   0         \n",
            "_________________________________________________________________\n",
            "conv_pw_10 (Conv2D)          (None, None, None, 512)   262144    \n",
            "_________________________________________________________________\n",
            "conv_pw_10_bn (BatchNormaliz (None, None, None, 512)   2048      \n",
            "_________________________________________________________________\n",
            "conv_pw_10_relu (ReLU)       (None, None, None, 512)   0         \n",
            "_________________________________________________________________\n",
            "conv_dw_11 (DepthwiseConv2D) (None, None, None, 512)   4608      \n",
            "_________________________________________________________________\n",
            "conv_dw_11_bn (BatchNormaliz (None, None, None, 512)   2048      \n",
            "_________________________________________________________________\n",
            "conv_dw_11_relu (ReLU)       (None, None, None, 512)   0         \n",
            "_________________________________________________________________\n",
            "conv_pw_11 (Conv2D)          (None, None, None, 512)   262144    \n",
            "_________________________________________________________________\n",
            "conv_pw_11_bn (BatchNormaliz (None, None, None, 512)   2048      \n",
            "_________________________________________________________________\n",
            "conv_pw_11_relu (ReLU)       (None, None, None, 512)   0         \n",
            "_________________________________________________________________\n",
            "conv_pad_12 (ZeroPadding2D)  (None, None, None, 512)   0         \n",
            "_________________________________________________________________\n",
            "conv_dw_12 (DepthwiseConv2D) (None, None, None, 512)   4608      \n",
            "_________________________________________________________________\n",
            "conv_dw_12_bn (BatchNormaliz (None, None, None, 512)   2048      \n",
            "_________________________________________________________________\n",
            "conv_dw_12_relu (ReLU)       (None, None, None, 512)   0         \n",
            "_________________________________________________________________\n",
            "conv_pw_12 (Conv2D)          (None, None, None, 1024)  524288    \n",
            "_________________________________________________________________\n",
            "conv_pw_12_bn (BatchNormaliz (None, None, None, 1024)  4096      \n",
            "_________________________________________________________________\n",
            "conv_pw_12_relu (ReLU)       (None, None, None, 1024)  0         \n",
            "_________________________________________________________________\n",
            "conv_dw_13 (DepthwiseConv2D) (None, None, None, 1024)  9216      \n",
            "_________________________________________________________________\n",
            "conv_dw_13_bn (BatchNormaliz (None, None, None, 1024)  4096      \n",
            "_________________________________________________________________\n",
            "conv_dw_13_relu (ReLU)       (None, None, None, 1024)  0         \n",
            "_________________________________________________________________\n",
            "conv_pw_13 (Conv2D)          (None, None, None, 1024)  1048576   \n",
            "_________________________________________________________________\n",
            "conv_pw_13_bn (BatchNormaliz (None, None, None, 1024)  4096      \n",
            "_________________________________________________________________\n",
            "conv_pw_13_relu (ReLU)       (None, None, None, 1024)  0         \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d_2 ( (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 1024)              4096      \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 512)               524800    \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 5)                 2565      \n",
            "=================================================================\n",
            "Total params: 3,760,325\n",
            "Trainable params: 3,736,389\n",
            "Non-trainable params: 23,936\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CLckhClwhrdt"
      },
      "source": [
        "es = EarlyStopping(monitor='val_loss',\n",
        "                   patience=5,\n",
        "                   mode='auto',\n",
        "                   restore_best_weights=True)\n",
        "checkpoint = ModelCheckpoint('best_weight_scene_mobileNet.h5',\n",
        "                             monitor='val_accuracy',\n",
        "                             verbose=1,\n",
        "                             save_best_only=True,\n",
        "                             mode='auto',)"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "naV9BLCupjdH"
      },
      "source": [
        "ma i"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T_B2Zm5XhuM_",
        "outputId": "12c39de7-57ea-4b2b-e8a6-31f198dafff2"
      },
      "source": [
        "# train the model using the Adam optimizer\n",
        "print(\"[INFO] training network...\")\n",
        "opt = Adam(lr=1e-3, decay=1e-3 / 50)\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=opt,\n",
        "\tmetrics=[\"accuracy\"])\n",
        "H = model.fit(trainX, trainY, validation_data=(testX, testY), epochs=200, batch_size=32, callbacks=[ checkpoint])"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] training network...\n",
            "Epoch 1/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.4783 - accuracy: 0.8773\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.59864, saving model to best_weight_scene_mobileNet.h5\n",
            "14/14 [==============================] - 79s 6s/step - loss: 0.4783 - accuracy: 0.8773 - val_loss: 2.4812 - val_accuracy: 0.5986\n",
            "Epoch 2/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.2834 - accuracy: 0.9273\n",
            "Epoch 00002: val_accuracy did not improve from 0.59864\n",
            "14/14 [==============================] - 78s 6s/step - loss: 0.2834 - accuracy: 0.9273 - val_loss: 2.1791 - val_accuracy: 0.5306\n",
            "Epoch 3/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.3053 - accuracy: 0.9341\n",
            "Epoch 00003: val_accuracy improved from 0.59864 to 0.63265, saving model to best_weight_scene_mobileNet.h5\n",
            "14/14 [==============================] - 80s 6s/step - loss: 0.3053 - accuracy: 0.9341 - val_loss: 2.4503 - val_accuracy: 0.6327\n",
            "Epoch 4/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.1070 - accuracy: 0.9727\n",
            "Epoch 00004: val_accuracy did not improve from 0.63265\n",
            "14/14 [==============================] - 80s 6s/step - loss: 0.1070 - accuracy: 0.9727 - val_loss: 5.2084 - val_accuracy: 0.4490\n",
            "Epoch 5/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.1429 - accuracy: 0.9545\n",
            "Epoch 00005: val_accuracy did not improve from 0.63265\n",
            "14/14 [==============================] - 78s 6s/step - loss: 0.1429 - accuracy: 0.9545 - val_loss: 4.0429 - val_accuracy: 0.5578\n",
            "Epoch 6/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.0516 - accuracy: 0.9773\n",
            "Epoch 00006: val_accuracy improved from 0.63265 to 0.74830, saving model to best_weight_scene_mobileNet.h5\n",
            "14/14 [==============================] - 79s 6s/step - loss: 0.0516 - accuracy: 0.9773 - val_loss: 1.7777 - val_accuracy: 0.7483\n",
            "Epoch 7/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.0609 - accuracy: 0.9886\n",
            "Epoch 00007: val_accuracy did not improve from 0.74830\n",
            "14/14 [==============================] - 79s 6s/step - loss: 0.0609 - accuracy: 0.9886 - val_loss: 1.5356 - val_accuracy: 0.7279\n",
            "Epoch 8/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.0364 - accuracy: 0.9864\n",
            "Epoch 00008: val_accuracy improved from 0.74830 to 0.78231, saving model to best_weight_scene_mobileNet.h5\n",
            "14/14 [==============================] - 80s 6s/step - loss: 0.0364 - accuracy: 0.9864 - val_loss: 1.2601 - val_accuracy: 0.7823\n",
            "Epoch 9/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.1244 - accuracy: 0.9750\n",
            "Epoch 00009: val_accuracy did not improve from 0.78231\n",
            "14/14 [==============================] - 79s 6s/step - loss: 0.1244 - accuracy: 0.9750 - val_loss: 1.9905 - val_accuracy: 0.7483\n",
            "Epoch 10/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.0415 - accuracy: 0.9909\n",
            "Epoch 00010: val_accuracy did not improve from 0.78231\n",
            "14/14 [==============================] - 79s 6s/step - loss: 0.0415 - accuracy: 0.9909 - val_loss: 2.8588 - val_accuracy: 0.7211\n",
            "Epoch 11/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.1231 - accuracy: 0.9727\n",
            "Epoch 00011: val_accuracy did not improve from 0.78231\n",
            "14/14 [==============================] - 78s 6s/step - loss: 0.1231 - accuracy: 0.9727 - val_loss: 3.4678 - val_accuracy: 0.6327\n",
            "Epoch 12/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.1246 - accuracy: 0.9682\n",
            "Epoch 00012: val_accuracy did not improve from 0.78231\n",
            "14/14 [==============================] - 78s 6s/step - loss: 0.1246 - accuracy: 0.9682 - val_loss: 9.4523 - val_accuracy: 0.4898\n",
            "Epoch 13/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.1962 - accuracy: 0.9523\n",
            "Epoch 00013: val_accuracy did not improve from 0.78231\n",
            "14/14 [==============================] - 78s 6s/step - loss: 0.1962 - accuracy: 0.9523 - val_loss: 8.1624 - val_accuracy: 0.4762\n",
            "Epoch 14/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.1301 - accuracy: 0.9750\n",
            "Epoch 00014: val_accuracy did not improve from 0.78231\n",
            "14/14 [==============================] - 78s 6s/step - loss: 0.1301 - accuracy: 0.9750 - val_loss: 4.5430 - val_accuracy: 0.6190\n",
            "Epoch 15/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.1860 - accuracy: 0.9591\n",
            "Epoch 00015: val_accuracy did not improve from 0.78231\n",
            "14/14 [==============================] - 78s 6s/step - loss: 0.1860 - accuracy: 0.9591 - val_loss: 11.0145 - val_accuracy: 0.4082\n",
            "Epoch 16/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.0835 - accuracy: 0.9705\n",
            "Epoch 00016: val_accuracy did not improve from 0.78231\n",
            "14/14 [==============================] - 78s 6s/step - loss: 0.0835 - accuracy: 0.9705 - val_loss: 8.4652 - val_accuracy: 0.4558\n",
            "Epoch 17/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.0459 - accuracy: 0.9864\n",
            "Epoch 00017: val_accuracy did not improve from 0.78231\n",
            "14/14 [==============================] - 79s 6s/step - loss: 0.0459 - accuracy: 0.9864 - val_loss: 3.0142 - val_accuracy: 0.7075\n",
            "Epoch 18/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.0272 - accuracy: 0.9932\n",
            "Epoch 00018: val_accuracy did not improve from 0.78231\n",
            "14/14 [==============================] - 78s 6s/step - loss: 0.0272 - accuracy: 0.9932 - val_loss: 2.9713 - val_accuracy: 0.7075\n",
            "Epoch 19/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.0168 - accuracy: 0.9955\n",
            "Epoch 00019: val_accuracy did not improve from 0.78231\n",
            "14/14 [==============================] - 78s 6s/step - loss: 0.0168 - accuracy: 0.9955 - val_loss: 6.4427 - val_accuracy: 0.5646\n",
            "Epoch 20/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.0647 - accuracy: 0.9909\n",
            "Epoch 00020: val_accuracy did not improve from 0.78231\n",
            "14/14 [==============================] - 78s 6s/step - loss: 0.0647 - accuracy: 0.9909 - val_loss: 5.7691 - val_accuracy: 0.6599\n",
            "Epoch 21/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.1239 - accuracy: 0.9864\n",
            "Epoch 00021: val_accuracy did not improve from 0.78231\n",
            "14/14 [==============================] - 78s 6s/step - loss: 0.1239 - accuracy: 0.9864 - val_loss: 4.8186 - val_accuracy: 0.6463\n",
            "Epoch 22/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.1518 - accuracy: 0.9705\n",
            "Epoch 00022: val_accuracy improved from 0.78231 to 0.85034, saving model to best_weight_scene_mobileNet.h5\n",
            "14/14 [==============================] - 79s 6s/step - loss: 0.1518 - accuracy: 0.9705 - val_loss: 0.8871 - val_accuracy: 0.8503\n",
            "Epoch 23/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.0432 - accuracy: 0.9909\n",
            "Epoch 00023: val_accuracy improved from 0.85034 to 0.94558, saving model to best_weight_scene_mobileNet.h5\n",
            "14/14 [==============================] - 79s 6s/step - loss: 0.0432 - accuracy: 0.9909 - val_loss: 0.3911 - val_accuracy: 0.9456\n",
            "Epoch 24/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.0519 - accuracy: 0.9864\n",
            "Epoch 00024: val_accuracy did not improve from 0.94558\n",
            "14/14 [==============================] - 77s 6s/step - loss: 0.0519 - accuracy: 0.9864 - val_loss: 0.6172 - val_accuracy: 0.9184\n",
            "Epoch 25/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.1538 - accuracy: 0.9682\n",
            "Epoch 00025: val_accuracy did not improve from 0.94558\n",
            "14/14 [==============================] - 78s 6s/step - loss: 0.1538 - accuracy: 0.9682 - val_loss: 2.3931 - val_accuracy: 0.8299\n",
            "Epoch 26/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.0203 - accuracy: 0.9886\n",
            "Epoch 00026: val_accuracy did not improve from 0.94558\n",
            "14/14 [==============================] - 81s 6s/step - loss: 0.0203 - accuracy: 0.9886 - val_loss: 0.4547 - val_accuracy: 0.9184\n",
            "Epoch 27/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.0310 - accuracy: 0.9909\n",
            "Epoch 00027: val_accuracy did not improve from 0.94558\n",
            "14/14 [==============================] - 78s 6s/step - loss: 0.0310 - accuracy: 0.9909 - val_loss: 0.2978 - val_accuracy: 0.9388\n",
            "Epoch 28/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 5.5289e-04 - accuracy: 1.0000\n",
            "Epoch 00028: val_accuracy did not improve from 0.94558\n",
            "14/14 [==============================] - 78s 6s/step - loss: 5.5289e-04 - accuracy: 1.0000 - val_loss: 0.6192 - val_accuracy: 0.9320\n",
            "Epoch 29/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.0205 - accuracy: 0.9955\n",
            "Epoch 00029: val_accuracy did not improve from 0.94558\n",
            "14/14 [==============================] - 78s 6s/step - loss: 0.0205 - accuracy: 0.9955 - val_loss: 0.7330 - val_accuracy: 0.9184\n",
            "Epoch 30/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.0170 - accuracy: 0.9955\n",
            "Epoch 00030: val_accuracy did not improve from 0.94558\n",
            "14/14 [==============================] - 78s 6s/step - loss: 0.0170 - accuracy: 0.9955 - val_loss: 0.6803 - val_accuracy: 0.9116\n",
            "Epoch 31/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.0236 - accuracy: 0.9932\n",
            "Epoch 00031: val_accuracy did not improve from 0.94558\n",
            "14/14 [==============================] - 78s 6s/step - loss: 0.0236 - accuracy: 0.9932 - val_loss: 1.1236 - val_accuracy: 0.8639\n",
            "Epoch 32/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.0370 - accuracy: 0.9909\n",
            "Epoch 00032: val_accuracy did not improve from 0.94558\n",
            "14/14 [==============================] - 79s 6s/step - loss: 0.0370 - accuracy: 0.9909 - val_loss: 1.2533 - val_accuracy: 0.8503\n",
            "Epoch 33/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.0168 - accuracy: 0.9955\n",
            "Epoch 00033: val_accuracy did not improve from 0.94558\n",
            "14/14 [==============================] - 78s 6s/step - loss: 0.0168 - accuracy: 0.9955 - val_loss: 0.9165 - val_accuracy: 0.8367\n",
            "Epoch 34/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.0188 - accuracy: 0.9932\n",
            "Epoch 00034: val_accuracy did not improve from 0.94558\n",
            "14/14 [==============================] - 78s 6s/step - loss: 0.0188 - accuracy: 0.9932 - val_loss: 1.5299 - val_accuracy: 0.8231\n",
            "Epoch 35/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.0108 - accuracy: 0.9932\n",
            "Epoch 00035: val_accuracy did not improve from 0.94558\n",
            "14/14 [==============================] - 79s 6s/step - loss: 0.0108 - accuracy: 0.9932 - val_loss: 0.7959 - val_accuracy: 0.9116\n",
            "Epoch 36/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.0292 - accuracy: 0.9955\n",
            "Epoch 00036: val_accuracy improved from 0.94558 to 0.95238, saving model to best_weight_scene_mobileNet.h5\n",
            "14/14 [==============================] - 79s 6s/step - loss: 0.0292 - accuracy: 0.9955 - val_loss: 0.3066 - val_accuracy: 0.9524\n",
            "Epoch 37/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.0271 - accuracy: 0.9886\n",
            "Epoch 00037: val_accuracy did not improve from 0.95238\n",
            "14/14 [==============================] - 79s 6s/step - loss: 0.0271 - accuracy: 0.9886 - val_loss: 0.3929 - val_accuracy: 0.9116\n",
            "Epoch 38/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.0528 - accuracy: 0.9841\n",
            "Epoch 00038: val_accuracy did not improve from 0.95238\n",
            "14/14 [==============================] - 79s 6s/step - loss: 0.0528 - accuracy: 0.9841 - val_loss: 2.5030 - val_accuracy: 0.7551\n",
            "Epoch 39/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.3047 - accuracy: 0.9614\n",
            "Epoch 00039: val_accuracy did not improve from 0.95238\n",
            "14/14 [==============================] - 80s 6s/step - loss: 0.3047 - accuracy: 0.9614 - val_loss: 4.5069 - val_accuracy: 0.6259\n",
            "Epoch 40/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.2952 - accuracy: 0.9341\n",
            "Epoch 00040: val_accuracy did not improve from 0.95238\n",
            "14/14 [==============================] - 78s 6s/step - loss: 0.2952 - accuracy: 0.9341 - val_loss: 1.4680 - val_accuracy: 0.8503\n",
            "Epoch 41/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.1161 - accuracy: 0.9773\n",
            "Epoch 00041: val_accuracy did not improve from 0.95238\n",
            "14/14 [==============================] - 78s 6s/step - loss: 0.1161 - accuracy: 0.9773 - val_loss: 2.8792 - val_accuracy: 0.7007\n",
            "Epoch 42/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.0830 - accuracy: 0.9841\n",
            "Epoch 00042: val_accuracy did not improve from 0.95238\n",
            "14/14 [==============================] - 79s 6s/step - loss: 0.0830 - accuracy: 0.9841 - val_loss: 3.6386 - val_accuracy: 0.6803\n",
            "Epoch 43/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.0503 - accuracy: 0.9909\n",
            "Epoch 00043: val_accuracy did not improve from 0.95238\n",
            "14/14 [==============================] - 78s 6s/step - loss: 0.0503 - accuracy: 0.9909 - val_loss: 3.4702 - val_accuracy: 0.6667\n",
            "Epoch 44/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.0571 - accuracy: 0.9818\n",
            "Epoch 00044: val_accuracy did not improve from 0.95238\n",
            "14/14 [==============================] - 78s 6s/step - loss: 0.0571 - accuracy: 0.9818 - val_loss: 1.4701 - val_accuracy: 0.8435\n",
            "Epoch 45/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.0266 - accuracy: 0.9932\n",
            "Epoch 00045: val_accuracy did not improve from 0.95238\n",
            "14/14 [==============================] - 79s 6s/step - loss: 0.0266 - accuracy: 0.9932 - val_loss: 0.8618 - val_accuracy: 0.9456\n",
            "Epoch 46/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.0193 - accuracy: 0.9932\n",
            "Epoch 00046: val_accuracy did not improve from 0.95238\n",
            "14/14 [==============================] - 81s 6s/step - loss: 0.0193 - accuracy: 0.9932 - val_loss: 1.0441 - val_accuracy: 0.8980\n",
            "Epoch 47/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.0255 - accuracy: 0.9909\n",
            "Epoch 00047: val_accuracy did not improve from 0.95238\n",
            "14/14 [==============================] - 79s 6s/step - loss: 0.0255 - accuracy: 0.9909 - val_loss: 1.5588 - val_accuracy: 0.8367\n",
            "Epoch 48/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.0900 - accuracy: 0.9841\n",
            "Epoch 00048: val_accuracy did not improve from 0.95238\n",
            "14/14 [==============================] - 80s 6s/step - loss: 0.0900 - accuracy: 0.9841 - val_loss: 2.5119 - val_accuracy: 0.8027\n",
            "Epoch 49/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.0554 - accuracy: 0.9864\n",
            "Epoch 00049: val_accuracy did not improve from 0.95238\n",
            "14/14 [==============================] - 80s 6s/step - loss: 0.0554 - accuracy: 0.9864 - val_loss: 1.2741 - val_accuracy: 0.8571\n",
            "Epoch 50/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.0730 - accuracy: 0.9841\n",
            "Epoch 00050: val_accuracy did not improve from 0.95238\n",
            "14/14 [==============================] - 79s 6s/step - loss: 0.0730 - accuracy: 0.9841 - val_loss: 1.2372 - val_accuracy: 0.8571\n",
            "Epoch 51/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.0567 - accuracy: 0.9841\n",
            "Epoch 00051: val_accuracy did not improve from 0.95238\n",
            "14/14 [==============================] - 79s 6s/step - loss: 0.0567 - accuracy: 0.9841 - val_loss: 0.6877 - val_accuracy: 0.9116\n",
            "Epoch 52/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.0584 - accuracy: 0.9795\n",
            "Epoch 00052: val_accuracy did not improve from 0.95238\n",
            "14/14 [==============================] - 79s 6s/step - loss: 0.0584 - accuracy: 0.9795 - val_loss: 0.9649 - val_accuracy: 0.8231\n",
            "Epoch 53/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.0587 - accuracy: 0.9818\n",
            "Epoch 00053: val_accuracy did not improve from 0.95238\n",
            "14/14 [==============================] - 80s 6s/step - loss: 0.0587 - accuracy: 0.9818 - val_loss: 2.1569 - val_accuracy: 0.7959\n",
            "Epoch 54/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.0458 - accuracy: 0.9886\n",
            "Epoch 00054: val_accuracy did not improve from 0.95238\n",
            "14/14 [==============================] - 79s 6s/step - loss: 0.0458 - accuracy: 0.9886 - val_loss: 2.3298 - val_accuracy: 0.7755\n",
            "Epoch 55/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.0131 - accuracy: 0.9955\n",
            "Epoch 00055: val_accuracy did not improve from 0.95238\n",
            "14/14 [==============================] - 79s 6s/step - loss: 0.0131 - accuracy: 0.9955 - val_loss: 1.1294 - val_accuracy: 0.8367\n",
            "Epoch 56/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.0111 - accuracy: 0.9955\n",
            "Epoch 00056: val_accuracy did not improve from 0.95238\n",
            "14/14 [==============================] - 80s 6s/step - loss: 0.0111 - accuracy: 0.9955 - val_loss: 0.3761 - val_accuracy: 0.9524\n",
            "Epoch 57/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.0111 - accuracy: 0.9955\n",
            "Epoch 00057: val_accuracy did not improve from 0.95238\n",
            "14/14 [==============================] - 79s 6s/step - loss: 0.0111 - accuracy: 0.9955 - val_loss: 0.6991 - val_accuracy: 0.9388\n",
            "Epoch 58/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.0309 - accuracy: 0.9955\n",
            "Epoch 00058: val_accuracy improved from 0.95238 to 0.96599, saving model to best_weight_scene_mobileNet.h5\n",
            "14/14 [==============================] - 80s 6s/step - loss: 0.0309 - accuracy: 0.9955 - val_loss: 0.3509 - val_accuracy: 0.9660\n",
            "Epoch 59/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.0106 - accuracy: 0.9955\n",
            "Epoch 00059: val_accuracy did not improve from 0.96599\n",
            "14/14 [==============================] - 80s 6s/step - loss: 0.0106 - accuracy: 0.9955 - val_loss: 0.3777 - val_accuracy: 0.9524\n",
            "Epoch 60/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 1.9146e-04 - accuracy: 1.0000\n",
            "Epoch 00060: val_accuracy did not improve from 0.96599\n",
            "14/14 [==============================] - 80s 6s/step - loss: 1.9146e-04 - accuracy: 1.0000 - val_loss: 0.4924 - val_accuracy: 0.9320\n",
            "Epoch 61/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.0023 - accuracy: 1.0000\n",
            "Epoch 00061: val_accuracy did not improve from 0.96599\n",
            "14/14 [==============================] - 79s 6s/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.4967 - val_accuracy: 0.9252\n",
            "Epoch 62/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 1.8624e-04 - accuracy: 1.0000\n",
            "Epoch 00062: val_accuracy did not improve from 0.96599\n",
            "14/14 [==============================] - 80s 6s/step - loss: 1.8624e-04 - accuracy: 1.0000 - val_loss: 0.6638 - val_accuracy: 0.9116\n",
            "Epoch 63/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 1.5613e-04 - accuracy: 1.0000\n",
            "Epoch 00063: val_accuracy did not improve from 0.96599\n",
            "14/14 [==============================] - 79s 6s/step - loss: 1.5613e-04 - accuracy: 1.0000 - val_loss: 0.6308 - val_accuracy: 0.9048\n",
            "Epoch 64/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 1.8488e-04 - accuracy: 1.0000\n",
            "Epoch 00064: val_accuracy did not improve from 0.96599\n",
            "14/14 [==============================] - 79s 6s/step - loss: 1.8488e-04 - accuracy: 1.0000 - val_loss: 0.5970 - val_accuracy: 0.9320\n",
            "Epoch 65/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.0035 - accuracy: 0.9977\n",
            "Epoch 00065: val_accuracy did not improve from 0.96599\n",
            "14/14 [==============================] - 78s 6s/step - loss: 0.0035 - accuracy: 0.9977 - val_loss: 0.7506 - val_accuracy: 0.9320\n",
            "Epoch 66/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 6.7462e-04 - accuracy: 1.0000\n",
            "Epoch 00066: val_accuracy did not improve from 0.96599\n",
            "14/14 [==============================] - 79s 6s/step - loss: 6.7462e-04 - accuracy: 1.0000 - val_loss: 0.8124 - val_accuracy: 0.9184\n",
            "Epoch 67/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 4.6562e-05 - accuracy: 1.0000\n",
            "Epoch 00067: val_accuracy did not improve from 0.96599\n",
            "14/14 [==============================] - 79s 6s/step - loss: 4.6562e-05 - accuracy: 1.0000 - val_loss: 0.7763 - val_accuracy: 0.9252\n",
            "Epoch 68/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.0034 - accuracy: 0.9977\n",
            "Epoch 00068: val_accuracy did not improve from 0.96599\n",
            "14/14 [==============================] - 80s 6s/step - loss: 0.0034 - accuracy: 0.9977 - val_loss: 0.4550 - val_accuracy: 0.9456\n",
            "Epoch 69/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 5.9793e-04 - accuracy: 1.0000\n",
            "Epoch 00069: val_accuracy did not improve from 0.96599\n",
            "14/14 [==============================] - 78s 6s/step - loss: 5.9793e-04 - accuracy: 1.0000 - val_loss: 0.4733 - val_accuracy: 0.9456\n",
            "Epoch 70/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.0056 - accuracy: 0.9955\n",
            "Epoch 00070: val_accuracy did not improve from 0.96599\n",
            "14/14 [==============================] - 79s 6s/step - loss: 0.0056 - accuracy: 0.9955 - val_loss: 0.5268 - val_accuracy: 0.9252\n",
            "Epoch 71/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.0179 - accuracy: 0.9977\n",
            "Epoch 00071: val_accuracy did not improve from 0.96599\n",
            "14/14 [==============================] - 79s 6s/step - loss: 0.0179 - accuracy: 0.9977 - val_loss: 0.9075 - val_accuracy: 0.8367\n",
            "Epoch 72/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.0211 - accuracy: 0.9955\n",
            "Epoch 00072: val_accuracy did not improve from 0.96599\n",
            "14/14 [==============================] - 78s 6s/step - loss: 0.0211 - accuracy: 0.9955 - val_loss: 0.5757 - val_accuracy: 0.8776\n",
            "Epoch 73/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.0131 - accuracy: 0.9932\n",
            "Epoch 00073: val_accuracy did not improve from 0.96599\n",
            "14/14 [==============================] - 78s 6s/step - loss: 0.0131 - accuracy: 0.9932 - val_loss: 3.5678 - val_accuracy: 0.6190\n",
            "Epoch 74/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.0155 - accuracy: 0.9955\n",
            "Epoch 00074: val_accuracy did not improve from 0.96599\n",
            "14/14 [==============================] - 78s 6s/step - loss: 0.0155 - accuracy: 0.9955 - val_loss: 7.9511 - val_accuracy: 0.4830\n",
            "Epoch 75/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.0309 - accuracy: 0.9932\n",
            "Epoch 00075: val_accuracy did not improve from 0.96599\n",
            "14/14 [==============================] - 79s 6s/step - loss: 0.0309 - accuracy: 0.9932 - val_loss: 3.3415 - val_accuracy: 0.6667\n",
            "Epoch 76/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.0172 - accuracy: 0.9932\n",
            "Epoch 00076: val_accuracy did not improve from 0.96599\n",
            "14/14 [==============================] - 78s 6s/step - loss: 0.0172 - accuracy: 0.9932 - val_loss: 0.8541 - val_accuracy: 0.8503\n",
            "Epoch 77/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.0081 - accuracy: 0.9955\n",
            "Epoch 00077: val_accuracy did not improve from 0.96599\n",
            "14/14 [==============================] - 77s 6s/step - loss: 0.0081 - accuracy: 0.9955 - val_loss: 0.6968 - val_accuracy: 0.9116\n",
            "Epoch 78/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.0247 - accuracy: 0.9932\n",
            "Epoch 00078: val_accuracy did not improve from 0.96599\n",
            "14/14 [==============================] - 78s 6s/step - loss: 0.0247 - accuracy: 0.9932 - val_loss: 1.0710 - val_accuracy: 0.8912\n",
            "Epoch 79/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.0549 - accuracy: 0.9932\n",
            "Epoch 00079: val_accuracy did not improve from 0.96599\n",
            "14/14 [==============================] - 78s 6s/step - loss: 0.0549 - accuracy: 0.9932 - val_loss: 0.8933 - val_accuracy: 0.8912\n",
            "Epoch 80/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.0305 - accuracy: 0.9932\n",
            "Epoch 00080: val_accuracy did not improve from 0.96599\n",
            "14/14 [==============================] - 78s 6s/step - loss: 0.0305 - accuracy: 0.9932 - val_loss: 1.8812 - val_accuracy: 0.8095\n",
            "Epoch 81/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.0969 - accuracy: 0.9795\n",
            "Epoch 00081: val_accuracy did not improve from 0.96599\n",
            "14/14 [==============================] - 78s 6s/step - loss: 0.0969 - accuracy: 0.9795 - val_loss: 2.1777 - val_accuracy: 0.7075\n",
            "Epoch 82/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.1120 - accuracy: 0.9659\n",
            "Epoch 00082: val_accuracy did not improve from 0.96599\n",
            "14/14 [==============================] - 79s 6s/step - loss: 0.1120 - accuracy: 0.9659 - val_loss: 1.3621 - val_accuracy: 0.8503\n",
            "Epoch 83/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.1064 - accuracy: 0.9659\n",
            "Epoch 00083: val_accuracy did not improve from 0.96599\n",
            "14/14 [==============================] - 78s 6s/step - loss: 0.1064 - accuracy: 0.9659 - val_loss: 6.8483 - val_accuracy: 0.6259\n",
            "Epoch 84/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.1657 - accuracy: 0.9682\n",
            "Epoch 00084: val_accuracy did not improve from 0.96599\n",
            "14/14 [==============================] - 78s 6s/step - loss: 0.1657 - accuracy: 0.9682 - val_loss: 7.2414 - val_accuracy: 0.5646\n",
            "Epoch 85/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.1563 - accuracy: 0.9614\n",
            "Epoch 00085: val_accuracy did not improve from 0.96599\n",
            "14/14 [==============================] - 78s 6s/step - loss: 0.1563 - accuracy: 0.9614 - val_loss: 9.7820 - val_accuracy: 0.4422\n",
            "Epoch 86/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.1727 - accuracy: 0.9705\n",
            "Epoch 00086: val_accuracy did not improve from 0.96599\n",
            "14/14 [==============================] - 78s 6s/step - loss: 0.1727 - accuracy: 0.9705 - val_loss: 9.3148 - val_accuracy: 0.5850\n",
            "Epoch 87/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.1420 - accuracy: 0.9682\n",
            "Epoch 00087: val_accuracy did not improve from 0.96599\n",
            "14/14 [==============================] - 78s 6s/step - loss: 0.1420 - accuracy: 0.9682 - val_loss: 9.7711 - val_accuracy: 0.5102\n",
            "Epoch 88/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.1449 - accuracy: 0.9727\n",
            "Epoch 00088: val_accuracy did not improve from 0.96599\n",
            "14/14 [==============================] - 79s 6s/step - loss: 0.1449 - accuracy: 0.9727 - val_loss: 4.9900 - val_accuracy: 0.6531\n",
            "Epoch 89/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.1386 - accuracy: 0.9636\n",
            "Epoch 00089: val_accuracy did not improve from 0.96599\n",
            "14/14 [==============================] - 79s 6s/step - loss: 0.1386 - accuracy: 0.9636 - val_loss: 5.7907 - val_accuracy: 0.6803\n",
            "Epoch 90/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.2040 - accuracy: 0.9659\n",
            "Epoch 00090: val_accuracy did not improve from 0.96599\n",
            "14/14 [==============================] - 79s 6s/step - loss: 0.2040 - accuracy: 0.9659 - val_loss: 7.4465 - val_accuracy: 0.5714\n",
            "Epoch 91/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.1504 - accuracy: 0.9682\n",
            "Epoch 00091: val_accuracy did not improve from 0.96599\n",
            "14/14 [==============================] - 79s 6s/step - loss: 0.1504 - accuracy: 0.9682 - val_loss: 5.9738 - val_accuracy: 0.6599\n",
            "Epoch 92/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.0720 - accuracy: 0.9727\n",
            "Epoch 00092: val_accuracy did not improve from 0.96599\n",
            "14/14 [==============================] - 79s 6s/step - loss: 0.0720 - accuracy: 0.9727 - val_loss: 17.5224 - val_accuracy: 0.4762\n",
            "Epoch 93/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.0292 - accuracy: 0.9886\n",
            "Epoch 00093: val_accuracy did not improve from 0.96599\n",
            "14/14 [==============================] - 79s 6s/step - loss: 0.0292 - accuracy: 0.9886 - val_loss: 13.0085 - val_accuracy: 0.4762\n",
            "Epoch 94/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.1201 - accuracy: 0.9773\n",
            "Epoch 00094: val_accuracy did not improve from 0.96599\n",
            "14/14 [==============================] - 79s 6s/step - loss: 0.1201 - accuracy: 0.9773 - val_loss: 6.9584 - val_accuracy: 0.6395\n",
            "Epoch 95/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.0360 - accuracy: 0.9932\n",
            "Epoch 00095: val_accuracy did not improve from 0.96599\n",
            "14/14 [==============================] - 79s 6s/step - loss: 0.0360 - accuracy: 0.9932 - val_loss: 1.0783 - val_accuracy: 0.8912\n",
            "Epoch 96/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.0276 - accuracy: 0.9841\n",
            "Epoch 00096: val_accuracy did not improve from 0.96599\n",
            "14/14 [==============================] - 79s 6s/step - loss: 0.0276 - accuracy: 0.9841 - val_loss: 0.4815 - val_accuracy: 0.9456\n",
            "Epoch 97/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.0364 - accuracy: 0.9977\n",
            "Epoch 00097: val_accuracy did not improve from 0.96599\n",
            "14/14 [==============================] - 80s 6s/step - loss: 0.0364 - accuracy: 0.9977 - val_loss: 0.4139 - val_accuracy: 0.9388\n",
            "Epoch 98/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.0225 - accuracy: 0.9955\n",
            "Epoch 00098: val_accuracy did not improve from 0.96599\n",
            "14/14 [==============================] - 80s 6s/step - loss: 0.0225 - accuracy: 0.9955 - val_loss: 0.6969 - val_accuracy: 0.9252\n",
            "Epoch 99/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.0132 - accuracy: 0.9977\n",
            "Epoch 00099: val_accuracy did not improve from 0.96599\n",
            "14/14 [==============================] - 80s 6s/step - loss: 0.0132 - accuracy: 0.9977 - val_loss: 0.2708 - val_accuracy: 0.9592\n",
            "Epoch 100/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 1.9126e-04 - accuracy: 1.0000\n",
            "Epoch 00100: val_accuracy improved from 0.96599 to 0.97279, saving model to best_weight_scene_mobileNet.h5\n",
            "14/14 [==============================] - 80s 6s/step - loss: 1.9126e-04 - accuracy: 1.0000 - val_loss: 0.2103 - val_accuracy: 0.9728\n",
            "Epoch 101/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 4.4061e-04 - accuracy: 1.0000\n",
            "Epoch 00101: val_accuracy did not improve from 0.97279\n",
            "14/14 [==============================] - 80s 6s/step - loss: 4.4061e-04 - accuracy: 1.0000 - val_loss: 0.1912 - val_accuracy: 0.9660\n",
            "Epoch 102/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 6.4777e-04 - accuracy: 1.0000\n",
            "Epoch 00102: val_accuracy did not improve from 0.97279\n",
            "14/14 [==============================] - 80s 6s/step - loss: 6.4777e-04 - accuracy: 1.0000 - val_loss: 0.1594 - val_accuracy: 0.9728\n",
            "Epoch 103/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.0064 - accuracy: 0.9977\n",
            "Epoch 00103: val_accuracy did not improve from 0.97279\n",
            "14/14 [==============================] - 80s 6s/step - loss: 0.0064 - accuracy: 0.9977 - val_loss: 0.2205 - val_accuracy: 0.9524\n",
            "Epoch 104/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 8.7011e-04 - accuracy: 1.0000\n",
            "Epoch 00104: val_accuracy did not improve from 0.97279\n",
            "14/14 [==============================] - 80s 6s/step - loss: 8.7011e-04 - accuracy: 1.0000 - val_loss: 0.2814 - val_accuracy: 0.9524\n",
            "Epoch 105/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.0048 - accuracy: 1.0000\n",
            "Epoch 00105: val_accuracy did not improve from 0.97279\n",
            "14/14 [==============================] - 80s 6s/step - loss: 0.0048 - accuracy: 1.0000 - val_loss: 0.1623 - val_accuracy: 0.9728\n",
            "Epoch 106/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 7.8615e-05 - accuracy: 1.0000\n",
            "Epoch 00106: val_accuracy improved from 0.97279 to 0.97959, saving model to best_weight_scene_mobileNet.h5\n",
            "14/14 [==============================] - 81s 6s/step - loss: 7.8615e-05 - accuracy: 1.0000 - val_loss: 0.1272 - val_accuracy: 0.9796\n",
            "Epoch 107/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 9.1125e-04 - accuracy: 1.0000\n",
            "Epoch 00107: val_accuracy improved from 0.97959 to 0.98639, saving model to best_weight_scene_mobileNet.h5\n",
            "14/14 [==============================] - 81s 6s/step - loss: 9.1125e-04 - accuracy: 1.0000 - val_loss: 0.0926 - val_accuracy: 0.9864\n",
            "Epoch 108/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 4.3027e-04 - accuracy: 1.0000\n",
            "Epoch 00108: val_accuracy did not improve from 0.98639\n",
            "14/14 [==============================] - 80s 6s/step - loss: 4.3027e-04 - accuracy: 1.0000 - val_loss: 0.0875 - val_accuracy: 0.9864\n",
            "Epoch 109/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 4.5513e-05 - accuracy: 1.0000\n",
            "Epoch 00109: val_accuracy did not improve from 0.98639\n",
            "14/14 [==============================] - 80s 6s/step - loss: 4.5513e-05 - accuracy: 1.0000 - val_loss: 0.0910 - val_accuracy: 0.9864\n",
            "Epoch 110/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.0025 - accuracy: 0.9977\n",
            "Epoch 00110: val_accuracy did not improve from 0.98639\n",
            "14/14 [==============================] - 80s 6s/step - loss: 0.0025 - accuracy: 0.9977 - val_loss: 0.0839 - val_accuracy: 0.9864\n",
            "Epoch 111/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 7.3140e-04 - accuracy: 1.0000\n",
            "Epoch 00111: val_accuracy did not improve from 0.98639\n",
            "14/14 [==============================] - 82s 6s/step - loss: 7.3140e-04 - accuracy: 1.0000 - val_loss: 0.0960 - val_accuracy: 0.9796\n",
            "Epoch 112/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 7.1438e-05 - accuracy: 1.0000\n",
            "Epoch 00112: val_accuracy did not improve from 0.98639\n",
            "14/14 [==============================] - 80s 6s/step - loss: 7.1438e-05 - accuracy: 1.0000 - val_loss: 0.0992 - val_accuracy: 0.9796\n",
            "Epoch 113/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 2.5585e-05 - accuracy: 1.0000\n",
            "Epoch 00113: val_accuracy did not improve from 0.98639\n",
            "14/14 [==============================] - 80s 6s/step - loss: 2.5585e-05 - accuracy: 1.0000 - val_loss: 0.0952 - val_accuracy: 0.9796\n",
            "Epoch 114/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 7.5593e-05 - accuracy: 1.0000\n",
            "Epoch 00114: val_accuracy did not improve from 0.98639\n",
            "14/14 [==============================] - 79s 6s/step - loss: 7.5593e-05 - accuracy: 1.0000 - val_loss: 0.0906 - val_accuracy: 0.9796\n",
            "Epoch 115/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 3.1478e-04 - accuracy: 1.0000\n",
            "Epoch 00115: val_accuracy did not improve from 0.98639\n",
            "14/14 [==============================] - 79s 6s/step - loss: 3.1478e-04 - accuracy: 1.0000 - val_loss: 0.0849 - val_accuracy: 0.9796\n",
            "Epoch 116/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.0114 - accuracy: 0.9977\n",
            "Epoch 00116: val_accuracy did not improve from 0.98639\n",
            "14/14 [==============================] - 79s 6s/step - loss: 0.0114 - accuracy: 0.9977 - val_loss: 0.1128 - val_accuracy: 0.9728\n",
            "Epoch 117/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.0017 - accuracy: 1.0000\n",
            "Epoch 00117: val_accuracy did not improve from 0.98639\n",
            "14/14 [==============================] - 79s 6s/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.3014 - val_accuracy: 0.9388\n",
            "Epoch 118/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.0556 - accuracy: 0.9841\n",
            "Epoch 00118: val_accuracy did not improve from 0.98639\n",
            "14/14 [==============================] - 81s 6s/step - loss: 0.0556 - accuracy: 0.9841 - val_loss: 0.1861 - val_accuracy: 0.9592\n",
            "Epoch 119/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.0090 - accuracy: 0.9932\n",
            "Epoch 00119: val_accuracy did not improve from 0.98639\n",
            "14/14 [==============================] - 80s 6s/step - loss: 0.0090 - accuracy: 0.9932 - val_loss: 2.0964 - val_accuracy: 0.7075\n",
            "Epoch 120/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.0130 - accuracy: 0.9932\n",
            "Epoch 00120: val_accuracy did not improve from 0.98639\n",
            "14/14 [==============================] - 80s 6s/step - loss: 0.0130 - accuracy: 0.9932 - val_loss: 0.7767 - val_accuracy: 0.8571\n",
            "Epoch 121/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.0141 - accuracy: 0.9977\n",
            "Epoch 00121: val_accuracy did not improve from 0.98639\n",
            "14/14 [==============================] - 81s 6s/step - loss: 0.0141 - accuracy: 0.9977 - val_loss: 0.2339 - val_accuracy: 0.9388\n",
            "Epoch 122/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.0305 - accuracy: 0.9932\n",
            "Epoch 00122: val_accuracy did not improve from 0.98639\n",
            "14/14 [==============================] - 81s 6s/step - loss: 0.0305 - accuracy: 0.9932 - val_loss: 0.3176 - val_accuracy: 0.9048\n",
            "Epoch 123/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.0079 - accuracy: 0.9977\n",
            "Epoch 00123: val_accuracy did not improve from 0.98639\n",
            "14/14 [==============================] - 80s 6s/step - loss: 0.0079 - accuracy: 0.9977 - val_loss: 0.4627 - val_accuracy: 0.9320\n",
            "Epoch 124/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.0148 - accuracy: 0.9977\n",
            "Epoch 00124: val_accuracy did not improve from 0.98639\n",
            "14/14 [==============================] - 80s 6s/step - loss: 0.0148 - accuracy: 0.9977 - val_loss: 0.6555 - val_accuracy: 0.8844\n",
            "Epoch 125/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.0129 - accuracy: 0.9955\n",
            "Epoch 00125: val_accuracy did not improve from 0.98639\n",
            "14/14 [==============================] - 80s 6s/step - loss: 0.0129 - accuracy: 0.9955 - val_loss: 0.3067 - val_accuracy: 0.9388\n",
            "Epoch 126/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.0100 - accuracy: 0.9955\n",
            "Epoch 00126: val_accuracy did not improve from 0.98639\n",
            "14/14 [==============================] - 79s 6s/step - loss: 0.0100 - accuracy: 0.9955 - val_loss: 0.2613 - val_accuracy: 0.9524\n",
            "Epoch 127/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.0102 - accuracy: 0.9977\n",
            "Epoch 00127: val_accuracy did not improve from 0.98639\n",
            "14/14 [==============================] - 79s 6s/step - loss: 0.0102 - accuracy: 0.9977 - val_loss: 0.2224 - val_accuracy: 0.9524\n",
            "Epoch 128/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.0061 - accuracy: 0.9977\n",
            "Epoch 00128: val_accuracy did not improve from 0.98639\n",
            "14/14 [==============================] - 79s 6s/step - loss: 0.0061 - accuracy: 0.9977 - val_loss: 0.1980 - val_accuracy: 0.9660\n",
            "Epoch 129/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.0020 - accuracy: 0.9977\n",
            "Epoch 00129: val_accuracy did not improve from 0.98639\n",
            "14/14 [==============================] - 79s 6s/step - loss: 0.0020 - accuracy: 0.9977 - val_loss: 0.1361 - val_accuracy: 0.9796\n",
            "Epoch 130/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.0042 - accuracy: 0.9977    \n",
            "Epoch 00130: val_accuracy did not improve from 0.98639\n",
            "14/14 [==============================] - 79s 6s/step - loss: 0.0042 - accuracy: 0.9977 - val_loss: 0.1135 - val_accuracy: 0.9728\n",
            "Epoch 131/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.0031 - accuracy: 0.9977\n",
            "Epoch 00131: val_accuracy did not improve from 0.98639\n",
            "14/14 [==============================] - 78s 6s/step - loss: 0.0031 - accuracy: 0.9977 - val_loss: 0.1469 - val_accuracy: 0.9660\n",
            "Epoch 132/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.0033 - accuracy: 0.9977\n",
            "Epoch 00132: val_accuracy did not improve from 0.98639\n",
            "14/14 [==============================] - 80s 6s/step - loss: 0.0033 - accuracy: 0.9977 - val_loss: 0.2535 - val_accuracy: 0.9524\n",
            "Epoch 133/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.0088 - accuracy: 0.9977\n",
            "Epoch 00133: val_accuracy did not improve from 0.98639\n",
            "14/14 [==============================] - 79s 6s/step - loss: 0.0088 - accuracy: 0.9977 - val_loss: 0.2654 - val_accuracy: 0.9456\n",
            "Epoch 134/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.0033 - accuracy: 0.9977\n",
            "Epoch 00134: val_accuracy did not improve from 0.98639\n",
            "14/14 [==============================] - 79s 6s/step - loss: 0.0033 - accuracy: 0.9977 - val_loss: 0.4342 - val_accuracy: 0.9320\n",
            "Epoch 135/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.0019 - accuracy: 1.0000\n",
            "Epoch 00135: val_accuracy did not improve from 0.98639\n",
            "14/14 [==============================] - 78s 6s/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.3922 - val_accuracy: 0.9388\n",
            "Epoch 136/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 2.8595e-04 - accuracy: 1.0000\n",
            "Epoch 00136: val_accuracy did not improve from 0.98639\n",
            "14/14 [==============================] - 79s 6s/step - loss: 2.8595e-04 - accuracy: 1.0000 - val_loss: 0.3510 - val_accuracy: 0.9388\n",
            "Epoch 137/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 7.3437e-04 - accuracy: 1.0000\n",
            "Epoch 00137: val_accuracy did not improve from 0.98639\n",
            "14/14 [==============================] - 78s 6s/step - loss: 7.3437e-04 - accuracy: 1.0000 - val_loss: 0.2696 - val_accuracy: 0.9592\n",
            "Epoch 138/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 3.9941e-05 - accuracy: 1.0000\n",
            "Epoch 00138: val_accuracy did not improve from 0.98639\n",
            "14/14 [==============================] - 79s 6s/step - loss: 3.9941e-05 - accuracy: 1.0000 - val_loss: 0.2382 - val_accuracy: 0.9524\n",
            "Epoch 139/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 4.8708e-05 - accuracy: 1.0000\n",
            "Epoch 00139: val_accuracy did not improve from 0.98639\n",
            "14/14 [==============================] - 78s 6s/step - loss: 4.8708e-05 - accuracy: 1.0000 - val_loss: 0.2187 - val_accuracy: 0.9524\n",
            "Epoch 140/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.0022 - accuracy: 0.9977\n",
            "Epoch 00140: val_accuracy did not improve from 0.98639\n",
            "14/14 [==============================] - 79s 6s/step - loss: 0.0022 - accuracy: 0.9977 - val_loss: 0.1977 - val_accuracy: 0.9592\n",
            "Epoch 141/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 1.8851e-04 - accuracy: 1.0000\n",
            "Epoch 00141: val_accuracy did not improve from 0.98639\n",
            "14/14 [==============================] - 79s 6s/step - loss: 1.8851e-04 - accuracy: 1.0000 - val_loss: 0.1855 - val_accuracy: 0.9592\n",
            "Epoch 142/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 5.8678e-05 - accuracy: 1.0000\n",
            "Epoch 00142: val_accuracy did not improve from 0.98639\n",
            "14/14 [==============================] - 79s 6s/step - loss: 5.8678e-05 - accuracy: 1.0000 - val_loss: 0.1761 - val_accuracy: 0.9592\n",
            "Epoch 143/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.0137 - accuracy: 0.9977    \n",
            "Epoch 00143: val_accuracy did not improve from 0.98639\n",
            "14/14 [==============================] - 79s 6s/step - loss: 0.0137 - accuracy: 0.9977 - val_loss: 0.2584 - val_accuracy: 0.9660\n",
            "Epoch 144/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.0416 - accuracy: 0.9955\n",
            "Epoch 00144: val_accuracy did not improve from 0.98639\n",
            "14/14 [==============================] - 79s 6s/step - loss: 0.0416 - accuracy: 0.9955 - val_loss: 0.3644 - val_accuracy: 0.9252\n",
            "Epoch 145/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.0229 - accuracy: 0.9977    \n",
            "Epoch 00145: val_accuracy did not improve from 0.98639\n",
            "14/14 [==============================] - 80s 6s/step - loss: 0.0229 - accuracy: 0.9977 - val_loss: 0.3097 - val_accuracy: 0.9388\n",
            "Epoch 146/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.1071 - accuracy: 0.9864\n",
            "Epoch 00146: val_accuracy did not improve from 0.98639\n",
            "14/14 [==============================] - 79s 6s/step - loss: 0.1071 - accuracy: 0.9864 - val_loss: 0.4705 - val_accuracy: 0.8912\n",
            "Epoch 147/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.0117 - accuracy: 0.9955\n",
            "Epoch 00147: val_accuracy did not improve from 0.98639\n",
            "14/14 [==============================] - 80s 6s/step - loss: 0.0117 - accuracy: 0.9955 - val_loss: 1.2446 - val_accuracy: 0.8299\n",
            "Epoch 148/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.0021 - accuracy: 1.0000\n",
            "Epoch 00148: val_accuracy did not improve from 0.98639\n",
            "14/14 [==============================] - 79s 6s/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 1.2810 - val_accuracy: 0.8095\n",
            "Epoch 149/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.0047 - accuracy: 0.9977\n",
            "Epoch 00149: val_accuracy did not improve from 0.98639\n",
            "14/14 [==============================] - 79s 6s/step - loss: 0.0047 - accuracy: 0.9977 - val_loss: 1.1557 - val_accuracy: 0.8095\n",
            "Epoch 150/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.0036 - accuracy: 0.9977\n",
            "Epoch 00150: val_accuracy did not improve from 0.98639\n",
            "14/14 [==============================] - 79s 6s/step - loss: 0.0036 - accuracy: 0.9977 - val_loss: 1.1341 - val_accuracy: 0.8299\n",
            "Epoch 151/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.0192 - accuracy: 0.9977\n",
            "Epoch 00151: val_accuracy did not improve from 0.98639\n",
            "14/14 [==============================] - 79s 6s/step - loss: 0.0192 - accuracy: 0.9977 - val_loss: 2.6494 - val_accuracy: 0.7755\n",
            "Epoch 152/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.0550 - accuracy: 0.9932\n",
            "Epoch 00152: val_accuracy did not improve from 0.98639\n",
            "14/14 [==============================] - 79s 6s/step - loss: 0.0550 - accuracy: 0.9932 - val_loss: 2.0430 - val_accuracy: 0.7959\n",
            "Epoch 153/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.0491 - accuracy: 0.9909\n",
            "Epoch 00153: val_accuracy did not improve from 0.98639\n",
            "14/14 [==============================] - 79s 6s/step - loss: 0.0491 - accuracy: 0.9909 - val_loss: 0.6282 - val_accuracy: 0.8776\n",
            "Epoch 154/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.0586 - accuracy: 0.9864\n",
            "Epoch 00154: val_accuracy did not improve from 0.98639\n",
            "14/14 [==============================] - 81s 6s/step - loss: 0.0586 - accuracy: 0.9864 - val_loss: 1.0355 - val_accuracy: 0.8435\n",
            "Epoch 155/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.0404 - accuracy: 0.9932\n",
            "Epoch 00155: val_accuracy did not improve from 0.98639\n",
            "14/14 [==============================] - 80s 6s/step - loss: 0.0404 - accuracy: 0.9932 - val_loss: 0.5891 - val_accuracy: 0.9184\n",
            "Epoch 156/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.0062 - accuracy: 0.9955\n",
            "Epoch 00156: val_accuracy did not improve from 0.98639\n",
            "14/14 [==============================] - 81s 6s/step - loss: 0.0062 - accuracy: 0.9955 - val_loss: 0.5329 - val_accuracy: 0.9252\n",
            "Epoch 157/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.0152 - accuracy: 0.9932\n",
            "Epoch 00157: val_accuracy did not improve from 0.98639\n",
            "14/14 [==============================] - 79s 6s/step - loss: 0.0152 - accuracy: 0.9932 - val_loss: 0.3431 - val_accuracy: 0.9184\n",
            "Epoch 158/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.0025 - accuracy: 1.0000\n",
            "Epoch 00158: val_accuracy did not improve from 0.98639\n",
            "14/14 [==============================] - 80s 6s/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.2445 - val_accuracy: 0.9252\n",
            "Epoch 159/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.0162 - accuracy: 0.9977\n",
            "Epoch 00159: val_accuracy did not improve from 0.98639\n",
            "14/14 [==============================] - 80s 6s/step - loss: 0.0162 - accuracy: 0.9977 - val_loss: 0.4315 - val_accuracy: 0.9048\n",
            "Epoch 160/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.0104 - accuracy: 0.9955\n",
            "Epoch 00160: val_accuracy did not improve from 0.98639\n",
            "14/14 [==============================] - 80s 6s/step - loss: 0.0104 - accuracy: 0.9955 - val_loss: 0.4010 - val_accuracy: 0.9116\n",
            "Epoch 161/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 7.9873e-05 - accuracy: 1.0000\n",
            "Epoch 00161: val_accuracy did not improve from 0.98639\n",
            "14/14 [==============================] - 79s 6s/step - loss: 7.9873e-05 - accuracy: 1.0000 - val_loss: 0.4439 - val_accuracy: 0.9116\n",
            "Epoch 162/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.0018 - accuracy: 1.0000\n",
            "Epoch 00162: val_accuracy did not improve from 0.98639\n",
            "14/14 [==============================] - 78s 6s/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.2722 - val_accuracy: 0.9388\n",
            "Epoch 163/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 3.2491e-04 - accuracy: 1.0000\n",
            "Epoch 00163: val_accuracy did not improve from 0.98639\n",
            "14/14 [==============================] - 78s 6s/step - loss: 3.2491e-04 - accuracy: 1.0000 - val_loss: 0.1559 - val_accuracy: 0.9524\n",
            "Epoch 164/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.0025 - accuracy: 0.9977\n",
            "Epoch 00164: val_accuracy did not improve from 0.98639\n",
            "14/14 [==============================] - 78s 6s/step - loss: 0.0025 - accuracy: 0.9977 - val_loss: 0.1575 - val_accuracy: 0.9524\n",
            "Epoch 165/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 2.3592e-05 - accuracy: 1.0000\n",
            "Epoch 00165: val_accuracy did not improve from 0.98639\n",
            "14/14 [==============================] - 78s 6s/step - loss: 2.3592e-05 - accuracy: 1.0000 - val_loss: 0.1698 - val_accuracy: 0.9524\n",
            "Epoch 166/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.0624 - accuracy: 0.9955\n",
            "Epoch 00166: val_accuracy did not improve from 0.98639\n",
            "14/14 [==============================] - 79s 6s/step - loss: 0.0624 - accuracy: 0.9955 - val_loss: 0.0630 - val_accuracy: 0.9728\n",
            "Epoch 167/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 8.7280e-04 - accuracy: 1.0000\n",
            "Epoch 00167: val_accuracy did not improve from 0.98639\n",
            "14/14 [==============================] - 81s 6s/step - loss: 8.7280e-04 - accuracy: 1.0000 - val_loss: 0.0861 - val_accuracy: 0.9660\n",
            "Epoch 168/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.0084 - accuracy: 0.9977    \n",
            "Epoch 00168: val_accuracy did not improve from 0.98639\n",
            "14/14 [==============================] - 81s 6s/step - loss: 0.0084 - accuracy: 0.9977 - val_loss: 0.0938 - val_accuracy: 0.9728\n",
            "Epoch 169/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.0029 - accuracy: 1.0000\n",
            "Epoch 00169: val_accuracy did not improve from 0.98639\n",
            "14/14 [==============================] - 80s 6s/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.1682 - val_accuracy: 0.9660\n",
            "Epoch 170/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.0011 - accuracy: 1.0000\n",
            "Epoch 00170: val_accuracy did not improve from 0.98639\n",
            "14/14 [==============================] - 79s 6s/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.1416 - val_accuracy: 0.9728\n",
            "Epoch 171/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 1.0180e-04 - accuracy: 1.0000\n",
            "Epoch 00171: val_accuracy did not improve from 0.98639\n",
            "14/14 [==============================] - 79s 6s/step - loss: 1.0180e-04 - accuracy: 1.0000 - val_loss: 0.1383 - val_accuracy: 0.9728\n",
            "Epoch 172/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.0034 - accuracy: 0.9977    \n",
            "Epoch 00172: val_accuracy did not improve from 0.98639\n",
            "14/14 [==============================] - 80s 6s/step - loss: 0.0034 - accuracy: 0.9977 - val_loss: 0.1427 - val_accuracy: 0.9728\n",
            "Epoch 173/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.0010 - accuracy: 1.0000\n",
            "Epoch 00173: val_accuracy did not improve from 0.98639\n",
            "14/14 [==============================] - 80s 6s/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.3059 - val_accuracy: 0.9524\n",
            "Epoch 174/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.0334 - accuracy: 0.9955\n",
            "Epoch 00174: val_accuracy did not improve from 0.98639\n",
            "14/14 [==============================] - 80s 6s/step - loss: 0.0334 - accuracy: 0.9955 - val_loss: 0.2617 - val_accuracy: 0.9728\n",
            "Epoch 175/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.0021 - accuracy: 1.0000\n",
            "Epoch 00175: val_accuracy did not improve from 0.98639\n",
            "14/14 [==============================] - 81s 6s/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 1.1825 - val_accuracy: 0.8639\n",
            "Epoch 176/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.0298 - accuracy: 0.9886\n",
            "Epoch 00176: val_accuracy did not improve from 0.98639\n",
            "14/14 [==============================] - 80s 6s/step - loss: 0.0298 - accuracy: 0.9886 - val_loss: 0.3971 - val_accuracy: 0.9388\n",
            "Epoch 177/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.1597 - accuracy: 0.9795\n",
            "Epoch 00177: val_accuracy did not improve from 0.98639\n",
            "14/14 [==============================] - 81s 6s/step - loss: 0.1597 - accuracy: 0.9795 - val_loss: 0.5264 - val_accuracy: 0.9116\n",
            "Epoch 178/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.0773 - accuracy: 0.9841\n",
            "Epoch 00178: val_accuracy did not improve from 0.98639\n",
            "14/14 [==============================] - 80s 6s/step - loss: 0.0773 - accuracy: 0.9841 - val_loss: 6.0128 - val_accuracy: 0.6395\n",
            "Epoch 179/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.1925 - accuracy: 0.9659\n",
            "Epoch 00179: val_accuracy did not improve from 0.98639\n",
            "14/14 [==============================] - 80s 6s/step - loss: 0.1925 - accuracy: 0.9659 - val_loss: 0.5937 - val_accuracy: 0.9252\n",
            "Epoch 180/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.1953 - accuracy: 0.9591\n",
            "Epoch 00180: val_accuracy did not improve from 0.98639\n",
            "14/14 [==============================] - 79s 6s/step - loss: 0.1953 - accuracy: 0.9591 - val_loss: 1.3222 - val_accuracy: 0.8367\n",
            "Epoch 181/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.0717 - accuracy: 0.9750\n",
            "Epoch 00181: val_accuracy did not improve from 0.98639\n",
            "14/14 [==============================] - 79s 6s/step - loss: 0.0717 - accuracy: 0.9750 - val_loss: 1.4970 - val_accuracy: 0.8571\n",
            "Epoch 182/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.0323 - accuracy: 0.9886\n",
            "Epoch 00182: val_accuracy did not improve from 0.98639\n",
            "14/14 [==============================] - 81s 6s/step - loss: 0.0323 - accuracy: 0.9886 - val_loss: 5.0679 - val_accuracy: 0.6259\n",
            "Epoch 183/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.0200 - accuracy: 0.9932\n",
            "Epoch 00183: val_accuracy did not improve from 0.98639\n",
            "14/14 [==============================] - 79s 6s/step - loss: 0.0200 - accuracy: 0.9932 - val_loss: 3.2585 - val_accuracy: 0.7211\n",
            "Epoch 184/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.1379 - accuracy: 0.9773\n",
            "Epoch 00184: val_accuracy did not improve from 0.98639\n",
            "14/14 [==============================] - 80s 6s/step - loss: 0.1379 - accuracy: 0.9773 - val_loss: 2.3830 - val_accuracy: 0.7007\n",
            "Epoch 185/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.0323 - accuracy: 0.9909\n",
            "Epoch 00185: val_accuracy did not improve from 0.98639\n",
            "14/14 [==============================] - 79s 6s/step - loss: 0.0323 - accuracy: 0.9909 - val_loss: 2.6608 - val_accuracy: 0.7075\n",
            "Epoch 186/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.0256 - accuracy: 0.9909\n",
            "Epoch 00186: val_accuracy did not improve from 0.98639\n",
            "14/14 [==============================] - 80s 6s/step - loss: 0.0256 - accuracy: 0.9909 - val_loss: 1.7265 - val_accuracy: 0.7891\n",
            "Epoch 187/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.0692 - accuracy: 0.9909\n",
            "Epoch 00187: val_accuracy did not improve from 0.98639\n",
            "14/14 [==============================] - 79s 6s/step - loss: 0.0692 - accuracy: 0.9909 - val_loss: 1.5409 - val_accuracy: 0.8299\n",
            "Epoch 188/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.0553 - accuracy: 0.9795\n",
            "Epoch 00188: val_accuracy did not improve from 0.98639\n",
            "14/14 [==============================] - 79s 6s/step - loss: 0.0553 - accuracy: 0.9795 - val_loss: 1.3533 - val_accuracy: 0.8163\n",
            "Epoch 189/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.0397 - accuracy: 0.9886\n",
            "Epoch 00189: val_accuracy did not improve from 0.98639\n",
            "14/14 [==============================] - 81s 6s/step - loss: 0.0397 - accuracy: 0.9886 - val_loss: 1.4294 - val_accuracy: 0.8503\n",
            "Epoch 190/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.0217 - accuracy: 0.9864\n",
            "Epoch 00190: val_accuracy did not improve from 0.98639\n",
            "14/14 [==============================] - 80s 6s/step - loss: 0.0217 - accuracy: 0.9864 - val_loss: 0.7511 - val_accuracy: 0.9184\n",
            "Epoch 191/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.0145 - accuracy: 0.9955\n",
            "Epoch 00191: val_accuracy did not improve from 0.98639\n",
            "14/14 [==============================] - 80s 6s/step - loss: 0.0145 - accuracy: 0.9955 - val_loss: 1.2849 - val_accuracy: 0.8367\n",
            "Epoch 192/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.0029 - accuracy: 0.9977\n",
            "Epoch 00192: val_accuracy did not improve from 0.98639\n",
            "14/14 [==============================] - 80s 6s/step - loss: 0.0029 - accuracy: 0.9977 - val_loss: 2.0267 - val_accuracy: 0.8231\n",
            "Epoch 193/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.0017 - accuracy: 1.0000    \n",
            "Epoch 00193: val_accuracy did not improve from 0.98639\n",
            "14/14 [==============================] - 80s 6s/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 1.6455 - val_accuracy: 0.8435\n",
            "Epoch 194/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 4.6321e-04 - accuracy: 1.0000\n",
            "Epoch 00194: val_accuracy did not improve from 0.98639\n",
            "14/14 [==============================] - 80s 6s/step - loss: 4.6321e-04 - accuracy: 1.0000 - val_loss: 1.3052 - val_accuracy: 0.8571\n",
            "Epoch 195/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.0011 - accuracy: 1.0000\n",
            "Epoch 00195: val_accuracy did not improve from 0.98639\n",
            "14/14 [==============================] - 79s 6s/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.8694 - val_accuracy: 0.8844\n",
            "Epoch 196/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.0067 - accuracy: 0.9977\n",
            "Epoch 00196: val_accuracy did not improve from 0.98639\n",
            "14/14 [==============================] - 79s 6s/step - loss: 0.0067 - accuracy: 0.9977 - val_loss: 0.7041 - val_accuracy: 0.9116\n",
            "Epoch 197/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.0455 - accuracy: 0.9955\n",
            "Epoch 00197: val_accuracy did not improve from 0.98639\n",
            "14/14 [==============================] - 80s 6s/step - loss: 0.0455 - accuracy: 0.9955 - val_loss: 0.7357 - val_accuracy: 0.9320\n",
            "Epoch 198/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.0288 - accuracy: 0.9955\n",
            "Epoch 00198: val_accuracy did not improve from 0.98639\n",
            "14/14 [==============================] - 78s 6s/step - loss: 0.0288 - accuracy: 0.9955 - val_loss: 1.9665 - val_accuracy: 0.7823\n",
            "Epoch 199/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.0211 - accuracy: 0.9932\n",
            "Epoch 00199: val_accuracy did not improve from 0.98639\n",
            "14/14 [==============================] - 78s 6s/step - loss: 0.0211 - accuracy: 0.9932 - val_loss: 1.8367 - val_accuracy: 0.7619\n",
            "Epoch 200/200\n",
            "14/14 [==============================] - ETA: 0s - loss: 0.0057 - accuracy: 0.9977\n",
            "Epoch 00200: val_accuracy did not improve from 0.98639\n",
            "14/14 [==============================] - 78s 6s/step - loss: 0.0057 - accuracy: 0.9977 - val_loss: 0.5921 - val_accuracy: 0.9184\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 590
        },
        "id": "l5wSsEwghwMr",
        "outputId": "26783874-149f-4917-8fe3-6af0dec5740f"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(H.history.keys())\n",
        "# summarize history for accuracy\n",
        "plt.plot(H.history['accuracy'])\n",
        "plt.plot(H.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "# summarize history for loss\n",
        "plt.plot(H.history['loss'])\n",
        "plt.plot(H.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO2deXgc1ZW339OLdsmWZXnfjQ1mNcYYE5ZhCWBMAgkwhDWQzUk+mISZhCyTbZKZ+ZJ8yWQhJBAIzkaAsITgCSYxa4AABmMMNrbBBox3y5atfWl19/3+uFXqVqtaakm9SN3nfR49VV1VXbpdXV2/e5Z7rhhjUBRFUZREfLlugKIoijI8UYFQFEVRPFGBUBRFUTxRgVAURVE8UYFQFEVRPFGBUBRFUTxRgVCUNCAivxGR/0rx2G0i8v6hnkdRMo0KhKIoiuKJCoSiKIriiQqEUjA4rp2bROR1EWkVkTtFZLyIPCoizSLyuIhUxx1/oYi8ISINIvK0iMyL23e8iKx13vdHoCThf31ARNY5731eRI4dZJs/JSJbReSgiKwQkUnOdhGRH4tInYg0ich6ETna2bdURDY6bdslIl8c1AVTCh4VCKXQuAQ4B5gLfBB4FPh3oBb7e/gcgIjMBe4BbnT2rQT+V0SKRKQI+DPwe2AMcL9zXpz3Hg8sBz4N1AC/BFaISPFAGioiZwHfBS4DJgLvAfc6u88FTnc+xyjnmHpn353Ap40xlcDRwJMD+b+K4qICoRQaPzPG7DPG7AKeBVYbY141xnQADwHHO8d9BHjEGPOYMaYL+CFQCrwPWAwEgZ8YY7qMMQ8AL8f9j2XAL40xq40xEWPMb4FO530D4SpguTFmrTGmE/gqcLKIzAC6gErgCECMMZuMMXuc93UBR4pIlTHmkDFm7QD/r6IAKhBK4bEvbr3d43WFsz4J22MHwBgTBXYAk519u0zPSpfvxa1PB77guJcaRKQBmOq8byAktqEFayVMNsY8CdwC/ByoE5HbRaTKOfQSYCnwnoj8XUROHuD/VRRABUJRkrEb+6AHrM8f+5DfBewBJjvbXKbFre8A/tsYMzrur8wYc88Q21COdVntAjDG3GyMOQE4EutqusnZ/rIx5iJgHNYVdt8A/6+iACoQipKM+4ALRORsEQkCX8C6iZ4HXgDCwOdEJCgiFwOL4t57B/AZETnJCSaXi8gFIlI5wDbcA3xMROY78Yv/i3WJbRORE53zB4FWoAOIOjGSq0RklOMaawKiQ7gOSgGjAqEoHhhj3gSuBn4GHMAGtD9ojAkZY0LAxcB1wEFsvOJPce9dA3wK6wI6BGx1jh1oGx4HvgE8iLVaZgOXO7ursEJ0COuGqgd+4Oy7BtgmIk3AZ7CxDEUZMKITBimKoiheqAWhKIqieKICoSiKoniiAqEoiqJ4ogKhKIqieBLIdQPSydixY82MGTNy3QxFUZQRwyuvvHLAGFPrtS+vBGLGjBmsWbMm181QFEUZMYjIe8n2qYtJURRF8UQFQlEURfFEBUJRFEXxJK9iEF50dXWxc+dOOjo6ct2UjFJSUsKUKVMIBoO5boqiKHlC3gvEzp07qaysZMaMGfQsvpk/GGOor69n586dzJw5M9fNURQlT8iYi0lEljvTIW6I2/ZHZxrGdc70j+uSvHebM4XiOhEZUlpSR0cHNTU1eSsOACJCTU1N3ltJiqJkl0xaEL/BVrP8nbvBGPMRd11E/gdo7OP9ZxpjDqSjIfksDi6F8BkVRckuGbMgjDHPYEsh98KZaOUybL17RRk57FwDe17LdSsUJSvkKovpNGCfMWZLkv0GWCUir4jIsr5OJCLLRGSNiKzZv39/2hs6VBoaGvjFL34x4PctXbqUhoaGDLRIGRJ/+3d4/Nu5boWiZIVcCcQV9G09nGqMWQCcD1wvIqcnO9AYc7sxZqExZmFtredo8ZySTCDC4XCf71u5ciWjR4/OVLOUwRLuhK72XLdCUbJC1rOYRCSAnY3rhGTHGGPcOXfrROQh7HSOz2SnhenlK1/5Cm+//Tbz588nGAxSUlJCdXU1mzdv5q233uJDH/oQO3bsoKOjg89//vMsW2YNJrdsSEtLC+effz6nnnoqzz//PJMnT+bhhx+mtLQ0x5+sQDERCGsygFIY5CLN9f3AZmPMTq+dzsTsPmNMs7N+LvCddPzjb//vG2zc3ZSOU3Vz5KQqvvXBo5Lu/973vseGDRtYt24dTz/9NBdccAEbNmzoTkddvnw5Y8aMob29nRNPPJFLLrmEmpqaHufYsmUL99xzD3fccQeXXXYZDz74IFdffXVaP4eSItEoRDtz3QpFyQqZTHO9Bzu5++EislNEPuHsupwE95KITBKRlc7L8cBzIvIa8BLwiDHmr5lqZ7ZZtGhRj7EKN998M8cddxyLFy9mx44dbNnSOywzc+ZM5s+fD8AJJ5zAtm3bstVcJRG1IJQCImMWhDHmiiTbr/PYthtY6qy/AxyXiTb11dPPFuXl5d3rTz/9NI8//jgvvPACZWVlnHHGGZ5jGYqLi7vX/X4/7e3qA88Z0QhEQrluhaJkBa3FlGEqKytpbm723NfY2Eh1dTVlZWVs3ryZF198McutUwaMWhBKAZH3pTZyTU1NDaeccgpHH300paWljB8/vnvfkiVLuO2225g3bx6HH344ixcvzmFLlZSIRmwmk6IUACoQWeDuu+/23F5cXMyjjz7quc+NM4wdO5YNG7qrlfDFL34x7e1TBoCJqkAoBYO6mBRlIEQjEOkEY3LdEkXJOCoQijIQTMQu1YpQCgAVCEUZCFFHICIqEEr+owKhKANBLQilgFCBUJSBEI3apaa6KgWACoSiDAS1IJQCQgUiwwy23DfAT37yE9ra2tLcImVIRFUglMJBBSLDqEDkGWpBKAWEDpTLMPHlvs855xzGjRvHfffdR2dnJx/+8If59re/TWtrK5dddhk7d+4kEonwjW98g3379rF7927OPPNMxo4dy1NPPZXrj6JAnAWhMQgl/yksgXj0K7B3fXrPOeEYOP97SXfHl/tetWoVDzzwAC+99BLGGC688EKeeeYZ9u/fz6RJk3jkkUcAW6Np1KhR/OhHP+Kpp55i7Nix6W2zMniMprkqhYO6mLLIqlWrWLVqFccffzwLFixg8+bNbNmyhWOOOYbHHnuML3/5yzz77LOMGjUq101VvHAzmEBdTEpBUFgWRB89/WxgjOGrX/0qn/70p3vtW7t2LStXruTrX/86Z599Nt/85jdz0EKlT1zrAdTFpBQEakFkmPhy3+eddx7Lly+npaUFgF27dlFXV8fu3bspKyvj6quv5qabbmLt2rW93qsMA6LxAqFzQij5T2FZEDkgvtz3+eefz5VXXsnJJ58MQEVFBXfddRdbt27lpptuwufzEQwGufXWWwFYtmwZS5YsYdKkSRqkHg6oBaEUGGLyqCrlwoULzZo1a3ps27RpE/PmzctRi7JLIX3WnNDRBN+batfP/wGctCy37VGUNCAirxhjFnrtUxeToqSKWhBKgaECoSipEp/FpGmuSgGQMYEQkeUiUiciG+K2/YeI7BKRdc7f0iTvXSIib4rIVhH5ylDbkk9utGQUwmfMOT0sCBUIJf/JpAXxG2CJx/YfG2PmO38rE3eKiB/4OXA+cCRwhYgcOdhGlJSUUF9fn9cPUGMM9fX1lJSU5Lop+U1UXUxKYZGxLCZjzDMiMmMQb10EbDXGvAMgIvcCFwEbB9OOKVOmsHPnTvbv3z+Yt48YSkpKmDJlSq6bkd8YTXNVCotcpLneICIfBdYAXzDGHErYPxnYEfd6J3BSspOJyDJgGcC0adN67Q8Gg8ycOXOobVYUtSCUgiPbQepbgdnAfGAP8D9DPaEx5nZjzEJjzMLa2tqhnk5RkqMxCKXAyKpAGGP2GWMixpgocAfWnZTILmBq3OspzjZFyS2axaQUGFkVCBGZGPfyw8AGj8NeBuaIyEwRKQIuB1Zko32K0idqQSgFRsZiECJyD3AGMFZEdgLfAs4QkfmAAbYBn3aOnQT8yhiz1BgTFpEbgL8BfmC5MeaNTLVTUVJGYxBKgZHJLKYrPDbfmeTY3cDSuNcrgV4psIqSU9SCUAoMHUmtKKkSVYFQCgsVCEVJFdeCEJ8KhFIQqEAoSqq4WUzBco1BKAWBCoSipIprQRSVaZqrUhCoQChKqrgxiGCpupiUgkAFQlFSxbUg1MWkFAgqEIqSKtE4F5MW61MKABUIRUkVE+9i6oA8LiGvKKACoSipE5/FhIFIV06boyiZRgVCUVIl3oIAzWRS8h4VCEVJlfgYBGgmk5L3qEAoSqrEZzGBZjIpeY8KhKKkSjTBxaQWhJLnqEAoSqoYJ0hd5FoQKhBKfqMCoSip0m1BuDEIdTEp+Y0KhKKkSmIWk1oQSp6jAqEoqZJoQWiaq5LnqEAoSqp0WxAldhkJ564tipIFVCAUJVVcCyLgDpTTekxKfpMxgRCR5SJSJyIb4rb9QEQ2i8jrIvKQiIxO8t5tIrJeRNaJyJpMtVFRBoSbxdRtQahAKPlNJi2I3wBLErY9BhxtjDkWeAv4ah/vP9MYM98YszBD7VOUgdHLgtBaTEp+kzGBMMY8AxxM2LbKGOM6bl8EpmTq/ytK2nFjEIFiu1QLQslzchmD+DjwaJJ9BlglIq+IyLK+TiIiy0RkjYis2b9/f9obqSjdJI6kVoFQ8pycCISIfA0IA39IcsipxpgFwPnA9SJyerJzGWNuN8YsNMYsrK2tzUBrFcWh24JwYhBRzWJS8pusC4SIXAd8ALjKGO8ZV4wxu5xlHfAQsChrDVSUZKgFoRQYWRUIEVkCfAm40BjTluSYchGpdNeBc4ENXscqSlZxs5gCmsWkFAaZTHO9B3gBOFxEdorIJ4BbgErgMSeF9Tbn2EkistJ563jgORF5DXgJeMQY89dMtVNRUqaXBaFZTEp+E8jUiY0xV3hsvjPJsbuBpc76O8BxmWqXogwaNwbhCwCiFoSS9+hIakVJlWgExA8i4C9SC0LJe1QgFCVVTAR8fruuAqEUACoQipIqrgUB4A+qi0nJe1QgFCVVTDTOglCBUPIfFQhFSZUeFoS6mJT8RwVCUVLFRMDn/GTUglAKABUIRUmVRAsiqhaEkt+oQChKqvTIYgqqi0nJe1QgFCVVotGEGIS6mJT8RgVCUVIl3oLwaQxCyX9UIDLF6tvhFi1Cm1dEIyDxQWp1MSn5jQpEpjj4Dhx407ollPyg10hqtSCU/EYFIlO4D4+u1ty2Q0kfOg5CKTBUIDKFmwLZ2ZzbdijpQ7OYlAJDBSJTRFQg8o4eWUwapFbyHxWITNEtEC25bYeSPnqMpFYXk5L/qEBkim4XU1Nu26GkD63mqhQYKhCZwu1dhtSCyBsSs5i01IaS56hAZAqNQeQfmsWkFBgZFQgRWS4idSKyIW7bGBF5TES2OMvqJO+91jlmi4hcm8l2ZgTNYso/dD4IpcDItAXxG2BJwravAE8YY+YATziveyAiY4BvAScBi4BvJROSYYtaEPlHvAWhpTaUAiCjAmGMeQY4mLD5IuC3zvpvgQ95vPU84DFjzEFjzCHgMXoLzfBGBSL/SMxiMlErGoqSp+QiBjHeGLPHWd8LjPc4ZjKwI+71TmdbL0RkmYisEZE1+/fvT29Lh4K6mPKPxCwmUCtCyWtyGqQ2xhjADPEctxtjFhpjFtbW1qapZWlAs5hGDtEItMUZuibJLZmYxQQaqFbymlwIxD4RmQjgLOs8jtkFTI17PcXZNnJQF9PI4ZVfw0/nQ0cTvPEQ/HAudLX3Pi4aAV/ArqtAKAVALgRiBeBmJV0LPOxxzN+Ac0Wk2glOn+tsGzm4rgcViOFPw3bobIR3nobX7oXWOmhv6H2ciS+14QiFupiUPCbTaa73AC8Ah4vIThH5BPA94BwR2QK833mNiCwUkV8BGGMOAv8JvOz8fcfZNnKIhu1SBWL4435HGx+2IgEQ6ex9XDQhSA0qEEpeE8jkyY0xVyTZdbbHsWuAT8a9Xg4sz1DT0s/Gh2HOuRAsta/VxTRycOtlbXiQ7pBY2OPBbxIGyoG6mJS8RkdSp4OG7XDfR2H9A7Ftbs9Sg9TDn24RjwtOhzt6HxdNKPcNWm5DyWtUINJBqM0uG3fGtqmLaeTQ2Qw1c+x6cZVdermOPC0IdTEp+UtKAiEinxeRKrHcKSJrReTcTDduxOA+JJp3x21zepZdbRAJZ79NSuqEmmHMLLjwFjjz3+22sFcMIqpprkpBkaoF8XFjTBM2m6gauAYnuKwQczM07Ylti4QgWGbX1c00vOlshuJKWHANTJxvt3kFqeMtCJ9mMSn5T6oCIc5yKfB7Y8wbcdsUtxfZvNcujbEPk9Ix9rW6mYY3rkAABIrt0tOC0CwmpbBIVSBeEZFVWIH4m4hUAtHMNWuEkehicgWjzKkvqBbE8KazBYor7HpfAqFZTEqBkWqa6yeA+cA7xpg2p9rqxzLXrBGGKxBt9fbB4j40Sh2BUAti+BLpgnB7LDjtdwTCyzLwymJSgVDymFQtiJOBN40xDSJyNfB1oDFzzRphxD8kmvfEYhJlNXap044OX1zx7nYxOZZByhaEupiU/CVVgbgVaBOR44AvAG8Dv8tYq0Ya8QLRtCfOgnBjEOpiGra47r8i18VUYpee4yCiakEoBUWqAhF2Kq9eBNxijPk5UJm5Zo0w4nuRzbvjYhAapB72JFoQfVkGJgLiBqm13LeS/6Qag2gWka9i01tPExEfEMxcs0YYiRZENNGCUIEYtvRyMfWXxaQuJqVwSNWC+AjQiR0PsRdbfvsHGWvVSKOHBRHvYhptl6HW7LdJSQ3X/ddtQfQRpNYsJqXASEkgHFH4AzBKRD4AdBhjNAbh4j5MiiqhKc7FFCwFxHvQlTI8cBMIXIHw+ex801qLSVFSLrVxGfAS8M/AZcBqEbk0kw0bUbh1l6qn28Fy7kPDX2T/1A0xfEl0MYF1MyVWc3UnP9QsJqWASDUG8TXgRGNMHYCI1AKPAw/0+a5CwX1IjJ4OdW/ELAhf0D5s1A0xfEnMYgJH1BOsvmjELl0LwqdZTEr+k2oMwueKg0P9AN6b/7gCUT7WTlvpPjT8QfvnFfBUhgdJLYgEF5NxBMLNYvL5se5DtSCU/CXVh/xfReRvInKdiFwHPAKszFyzRhiuIBRX2geL+9DwB23QUx8imaOrA9Yst2MUBkNnMwTLY5YBeLuYEi0IEXUfKnlPSi4mY8xNInIJcIqz6XZjzEOZa9YIIxKyLodgmVPeO87F5A/qQySTvP0k/OVfYdICmDR/4O/vbI7VYXLxF/d2MXVbEHFC4i/SUu5KXpPylKPGmAeBBzPYlpFLpMs+LNzpRl2/tj+ovcxM0+VM1tTVPrj3x1dydQkU9W9BgIq/kvf06WISkWYRafL4axaRQRUYEpHDRWRd3F+TiNyYcMwZItIYd8w3B/O/skakyz4sXIHocMpU+TVInXHcWMFgU4lDLb0Fwu8Vg3BcWKICoRQOfVoQxpi0l9MwxryJrQyLiPiBXYCXu+pZY8wH0v3/M0Ik1FMg3MCnv0iD1JnGfZAP9hp3NvfMYAJbjynxwe9pQRSp+Ct5Ta4zkc4G3jbGvJfjdgyNbheTM4OcO/jKF1AXU6ZxhWEoAuGW+nYJFPU+X2IWE6gFoeQ9KccgMsTlwD1J9p0sIq8Bu4EvOrPY9UJElgHLAKZNm5aRRvaLa0G4lUA7HIHojkFoLzNjpMOC8HIxxbusnvzv2BSjvSwIFQglf8mZQIhIEXAh8FWP3WuB6caYFhFZCvwZmON1HmPM7cDtAAsXLjQZam7fRJNYEO5I6q6GnDSrIHCFYbAxCK8spkQL4tW7YsckxiCimsWk5C+5dDGdD6w1xuxL3GGMaTLGtDjrK4GgiIzNdgNTptvF5FoQTpC6eyS19jIzRreLyaN2Uip4ZjGVxM5rDLQdsFV6QS0IpaDIpUBcQRL3kohMEBFx1hdh21mfxbYNjF5BateCCDh+anUxZYxugRjEg/rAFmv9VU3uuT3+wd/ZbNdDTuJBvAXh0xiEkt/kxMUkIuXAOcCn47Z9BsAYcxtwKfBZEQkD7cDlzoRFw5P4gXIQF4NwXEyaxZQ5umMQg7AgNjwICByRkCwXKI59Z20J/ZJe4yBU/JX8JScCYYxpBWoStt0Wt34LcEu22zVoImErBAEPF5Nfx0FklO4YxAB78sZYgZhxKlRN7LnP34dA9MhiKooN1FOUPCTXaa75QbeLyQ1SO+4In9/pZaoFkTEGa0HsXQ8H3oKjL+69LxCXxdR6oOc+HQehFBAqEOkgEuoZpO5ssq+1oFvmGWwM4s1HrTUw76Le+wLFNjspGrUB6nh6jaRWgVDyFxWIdNBdasOxINyYBNiUSX2IZI7BWhDth+wMgOU1vfd1TwbU2Y8FoUFqJb9RgUgHrovJH4ybccwJ72iQOrMMdhyE+515EXDmpQ53WAsi3mroVc1VxV/JX1Qg0oE7UA5iVoT72l9syzS4tXyU9DLYkdSRzth3lEi3QISgtR4qJ8a+V5+W2lAKBxWIdOC6mCA2FsJ1Mbnb3Z5maz28fn9225fPDLYWU6TLuv+88DsCEem0FkR5DVSMs9t6WRAqEEr+ogKRDtwgNcQC1a4wxPuzAdbfD3/6JNS/nd025iuDtiBCKVoQB6CsBsodgUjMYtJSG0oeowKRDnoIhOticoPUbm/UsSDcyYR2v5q99uUzg41BhPsQCHe7G4MoG5vEglAXk5LfqECkg0hXrNqnO1iul4vJeZC4M5/tWZe99uUzGbEgnO8w0mldguVxAuHTUhtK4ZDrct/5QcQrSJ3gYnIfYK5A7FaBSAuDjkH0JRDO9o5G6Gq1LibXlZQYgzBRm4AQLxyKkieoBTFUjEnIYnKC1L1iEI6LyS3NsOc1OxBLGRqRoQhEkjRXN0jdtNsue1gQCVlM7rmUwqOrHVZ9A0L5W25FBWKouA/+pFlMCUFq14LobIKD72SnjfmKMYOfkzoSisWHEgkkCETZWBgzy66XjI4dlyj+SmGx82V4/mbYsTrXLckYKhBDxe09JrMguoPUbgyiLSYeGocYGvE990xkMTXtssvysTDzn+CGNVAzO3acCkRh45Z3yWMLUgViqHQLhCsIiWmuCeMgutph3BH2OM1kGhrx5TUGWmojnIKLqWGHXZbX2rpaYxMmNXRHy+fxA0Lpg4gKhNIfbvCy28XkjrjtI0hdPApq5thqosrgca+pLzjwYn2RUEwIEnGD1HvX23OPTjLXebcFkb8PCKUP3O89j0vpqEAMlV4upkQLImEcRFebdUONmakxiKHiWg0lowZuQcRnniXifmetddZqSGppqIupoOm2IPL3+1eBGCq9BCIxzdVjHESw1PqyD22zkw0pg8PtuZVU2SD1QCYdjHT2X6wPoPaI5OfQLKbCplsg1IJQkpEsi6m7WF9iFlObFZExs6x7qnFH9tqab7hWQ3GVXQ7kQZ1KFhPAuCOTn8P9bqP524NU+kAtCKVfEi2IQEKaa2KpDdeCGONkwxzUmkyDJt6CiH+dCvEFFhOJj02M68uCUBdTQaNZTJlDRLaJyHoRWSciazz2i4jcLCJbReR1EVmQtca1HYTnb0nNZeE+HHyJFoQ7H0SiiynOggA4+G562lyIxMcgYGACEe6j3Lc/EJt7unZe8nP4NIupoNEgdcY50xgz3xiz0GPf+cAc528ZcGvWWrX5L7Dqa1C3qf9j+3UxuZVBHR+5G6SunGCFQqu6Dp5uF5MjEKn6gqMRO0dHsiwmsGnI/mKbTJAMzWIqbNTFlFMuAn5nLC8Co0VkYlb+c6dTcbVxZ//HJhso5zUfRCRka/cES21e/ZhZmsk0FLpdTAO0IBJF3Qt/EdTO7bvGkrqYCptkQerOZvjpfNj+YvbblGZyKRAGWCUir4jIMo/9k4H4CO5OZ1vmcUtypxJATjqSOtBze6QzVofJzXRSgRga3S6mAcYg3B90MhcTWNGZcGzf59EspsIm2UC55r1w6N28GAiby2qupxpjdonIOOAxEdlsjHlmoCdxxGUZwLRpSQY0DZTQQCyIhN5oIMHFFF9qw63D5IrImFnw5qM21dWvhXUHTC8LIsWxEO53liyLCeCKe22Jjb5QC6Kwcb/3xO/f7Qi21We3PRkgZxaEMWaXs6wDHgIWJRyyC5ga93qKsy3xPLcbYxYaYxbW1tamp3EDcTFF+ynW1x3I7IoTCNeCmGnf37x76G0uRFyBGGiaa2J5FC/GHxmr4JqMxDIqSmGRrNS8+ztvO5jd9mSAnAiEiJSLSKW7DpwLbEg4bAXwUSebaTHQaIzZk5UGDsiC6MfFJGL3heNdTM4xxZXO/8vfcsEZZbAWRDgFF1MqqIupsEkWpHYFon3kC0Su/BrjgYdExG3D3caYv4rIZwCMMbcBK4GlwFagDfhY1loXarXLpkG4mBKzmMBmw3hZEK47Ktw+tPYWKr1iEKlaEO53NlSB0CymgqbbxZS/FkROBMIY8w5wnMf22+LWDXB9NtvVTWezXTbt7n+2sEQLorgSECgqjx3jzl2caEG4dZu6BlhHaLCsu9um184+Kzv/L9MkuphSjkEkfGeDRWMQhU2yILX7O88DC2I4p7nmDteCiIahZV/fxyY+bEqr4doVcMxlsWMCxU4WU0KQ2i0Nni0L4unvwst3Zud/ZYNwh431uBZZyjGINLuYtNRGYeLeR8lcTHlgQahAeBFqiT10+otDuMX2fHHG2MzTobgi9tofdFxMCWmugSxbEO2NMfHLB8Kd9hq62UgDzmJSF5MyBNz7KDFI7d6HKhB5SmcLjJ1r1/sbC5GKu8Jf5J3mGsxiDCIagc58E4gOKw7dlliq4yDS5GLyaRZTQdOfiyncHvvNj1BUILwItcTKPPdrQaQiEMVOFlNikHqAD7ah0NFol115lDEV7nQEImFSpn7fly6B8AOiFkShkqxYX7wojHArQgUiEWOsQFRNssHPxl5DL3qSUtmGRBdTggWRjV5GR4Nduim8+UCiBZFqLaZ0WRBuCrMKRGHSnwUBIz5QrQKRSCRkg9NF5VA5sf9BbJGQdTXYlF1v/EX9BKmzEINwLe5W4fEAACAASURBVIh8GnMR7ogV1YPsu5jcc6iLqTBJOg4i7vc8wkdTq0Ak4o6iLq60A7A6mvo+PtrH1JUugeKYBeEvjqXNdgeps2BBtDsWRD66mHw+Z17qXAhEUAWiUElW7jv+N6Yupjwj5IyBKKqwAtHZj0BEuvqvo9Q9DqI9Zj2Ak30jWbIgXBdT68Cm5hzOuBYE2Gs5UIEYahYTxL5bpfBI6mJqt88PUBdT3uFm+RSV2xG6rmsmGZFQ/z3R7iB1WyxADdYtFSjJrgWBGfGZFd24FgTExpqkgrqYlHTQl0BUTbLrbYey26Y0owKRSLeLqcIGqftzMaUkEG6QOsGCADuaOitZTA2xdVcEG3bAHWeN3LLEkc6YBeEvHkAtpnS7mNSCKEiSZTGF2633oahCLYi8w83yiXcx9eWS6WtuY5f4cRDxFgTYekzZGAfRHicQXY5AvPsM7HoF7r0KWuoy34Z009XRs6x6zoLUKhAFSXcMwsOCCJZC6RiNQeQdPQSiyt4EffVM3XIPfREojtVi8rIgsjGSuocF4QTR9r1hH3BtB2HlTZlvQ7rpaov5enMmEEGb9aYUHt3F+jzSXAOlUFY94rOYdJaaRFwXU1F5rAhcR1PvB7tL3Saomd33OZMFqcGxILIgEO0eLqZ9G2D8UTBqCux/M/NtSDehVihyBx0ORiD6EfZUUAuicOmuxRSyXgY31d39nZfVqIsp73AfnsWVUDLarifLZGo7CAfegikn9n3ObhdTm4eLqThLA+Xigu2ui2nfG1YgSkZD+wgMpoVaY1VzA6Wpp/C6caO+xq6kik9jEAWJMfZ7Fx9gelqRXR32d64upjykR5qra0EkyWTa9YpdTk2cDC8Bf5H1U3oGqbNkQXQ0QLkz416ozcYc2g7A+KNtBdp4F9RIIBK2PbigIxDFlamPEo+kMHYlVXQcRGHiCoLr4ozvJLiu5MoJ0LwHotHsty9NqEAkEmoF8duefXE/ArHjJduDmLSg73P2GaQuyZ6LqWqyXQ+1WvcSWAuidLRtw0hKf+2KS0cGKxDuPB79Ee5Mj3sJ1MVUqLjuTFcg4t2bbkew5jD7u0pl4rFhigpEIp0tNsVVJDaVZTIX086X7AM2vrS3F4FiO+K69YC3BZGtILUrEF2t1r0EMO6omCutfQRZEaEhCEQkFCvPMVRUIAoT9zt3f/uuFWlMzIJwK0IfeCv77UsTKhCJhFpivYK+XEzRCOx8Bab0414COHypjVNEOmH01J77AiWZT3ONRu1ncAfvhByBqJgA5TXWxQQjy83kZmINSiDS7WLSLKaCwxWERBdTJASYBIHYmvXmpQvNYkokXiDis5gSOfCWjVf0F6AGmHgsfPJxe56iBGsjG2muoWYw0TiBaLNZS+OckualrgUxggLVobhsM7DfVVebfVj3V/okkk4XkwapCxL3O3fvP/e1mygRKIXysdYLoRZEHtHZEvvSiyoA8XYx7d9sl+OPSv3cJVW2sFw82bAgXNdRea3tOXe12qlUXZeTa0GMJBdTl4cFAbEkg76IhGIlOoaKltooTLpdTJU9X8dXbBaxVkT9luy3L01kXSBEZKqIPCUiG0XkDRH5vMcxZ4hIo4isc/6+mbUGhlpjfkWfz6nH5GVBOF96zWFD+3+BLFgQruuoZJQNkne2WIGoGOdsH93zuJGAG4MIJghEKm6mVEa/p4paEJlh+2p46v/muhXJ6bYgEoLUiZOCjZ1rnxVvPAQPfjK7bUwDubAgwsAXjDFHAouB60XkSI/jnjXGzHf+vpPxVrmpaPEuJoDiUd4xiANvwahpsYFag8VNc81khVW3/aWjbY+7cYdN06uYENsOyV1M7Q3DL1Wvl4tpAAIR7kxjDEKD1Bnh9T/C378PLftz3RJvkgWpE+d8qTnMprqu/BKsv7//4p/DjKwLhDFmjzFmrbPeDGwCJme7Hd10NMGd58LLd9jXnc09BaKkytvFdOAtGDtn6P8/UAKYzD5kXNdRiSMQB9+xryvH22XxKEC8XUztDfDjo2DDA5lr32DoDlI7Aj1gC0JdTMOaVqc22M6XctuOZIQTLAgvFxPEAtXu56l/OzvtSxM5jUGIyAzgeGC1x+6TReQ1EXlURJI6+kVkmYisEZE1+/cPordRUmV77y/+wj44m3bZ0hPd+z0mDYpGrdnofvlDwWva0dYDcNel6es9ua6j0tHW9D20zb6ucATC53M+p4dA1G+1vXU3LTYZXRm2ghLpTnNNSCjoTGGwXCSUXhdTVAUi7bj3/g6vR8MwINHF5JbdSJxW2H1GVE60SxWI1BCRCuBB4EZjTGIXfS0w3RhzHPAz4M/JzmOMud0Ys9AYs7C2tnZwjTn5evvQvPsjduDbok/F9hV7zAnRvNveCGmzIOg5WG7nGtj6GOxeO/TzQ4IFURG7uV2BACseXi4m19po6mPq1ea98LMF8MS309PeVPAaKAf9T/AEThZTmlxMWmojM7g97h0v57YdyUjmYnJ/x65AjJkFx34ELr4DEDioAtEvIhLEisMfjDF/StxvjGkyxrQ46yuBoIiMzViD5n0QRk+3bqMTroulg4LjYkoQCDdtLVMWhFsBMl1ppx0NdnR4cWXPmEkPgaj2djG5PZ5kAhHpgvuvs5ZXf1ZGOgm1AhIT2FRcTE27bXmUSFd6ZpMDKzQmasfFKOnDtSB2vzo8XXiJaa7hBAsi4Pyu/QG4+HaYeZr1TKgF0TciIsCdwCZjzI+SHDPBOQ4RWYRtZ+bq5vr8cNoXrC/+1H/tuc/LxeRmMKVDINx0y3gLwq0AmS6BaG+wn0Mkll1RVNFzBHiygn3dFsQu73O/di9sf8Gm0DYmOSYThFrtZ3AL7qUiEH9aZl13aQ1SO66q4WRFtOyHN/+a61YMnq52m6484RibAr53fa5b1JteLqYkMYh4xsxSCyIFTgGuAc6KS2NdKiKfEZHPOMdcCmwQkdeAm4HLjcmwg/uEa+FLb/e0HsC6mBInDTrwlhUTN010KLg9jXiByIQF4WYquTd0vPUAyQv2xbuYvL6C+i32YXvkRdmtORNf6htinyuZQOxaC9ueteLbtDu9WUwwvATilV/DPZenFo8ZjriTVx3xAbvcuSZ3bUlGr3EQbhaTG4PwyG6smZ2aBfHynbDpf/s/Lgsxv6yPpDbGPAf0WWfZGHMLcEt2WhSHV+CypMq6EEItsZth/5s2/pCOctFBx0USPxbCLRGcroFr7Q2xsQ7uQ7WXQIz2/n8H3wZfwPrt2w7a0hzxNO6yojpqio3VuLWsMk18qW+wgfaiiuQC8fzNsfVwewYEYhiV22g9ABgbGyoe4jidXNB6wC4nzrfjXNykiuFErywm18XkxiBKer+n5jDbCWs7CGVjvM9rDDz5n1BUCYdf0HtgrcsT/wlv/Q2uuh+qJg7+c/SDjqTuD7dgn+tmMsaavBOOTs/5uy2ITMYgGmOfw+3ZVCYIhOtiiu+VtB202yafYF97uZmadkHVFPuX7JhM0NXWUyDAqcfkEaRu2Q8bH4bjroxtS5tAOH2s4WRBuPdN857ctmOwuAHqilprpbfszW17vOh3HISHBTHGmVisvo/aTE277ffXuL3vFN8dq2Hfevj1koxOF6wC0R9u+mTLPrts2mV7AePTJBBeFoT7A8+oi2lCz2NKq8FEes6pcOhdu5xxql16Baobd8GoyTHXXGOW3EyhltgoapdkBfv2b7ZW4LGX2UlcIL9dTCNdINwHXvk4O6fCcJwvPelI6jZrcXt5I9yZJ/tyM7ll+AFevy/5cU27oPYIa11tWpFysweKCkR/TDvZ9vKf/5l97QbMJhybnvNnw4LwdDElxE+6R1PHuZnqnfjDjNPsMtE6iEYcC2KyFQnoOx02nSS6mCC5QLhCN2am/VFBerOYYHhl2rhJDiNVIFwLotyxIJqHsQXhFaT2sh7AZkoGy+yI6mSVCVyBOOwcW57D674yxv7OZp9txSiDvzkViP6omginfA7e+JPNyd7rTrTjVR1kELhZTF1pCFIb0zvd0pgEC8J5qFYmWBDdc0LE/c+D7wBiZ8wTf2+BaNlnrY5RU6Bykj02Wy6mUFvvMifJBOLgu/aHVDUlVsE2n7OYui2IYfhgTYWW/TYJJFhiLd1hbUEkVHMNt8dSrxMJFME534G3n4DVt3ofs3cDjJ5mx2K1H4S3PLLR2g/ZpJbRU+0APBWIHPO+z9kb9bFvwt7XbbqaG7AeKsGELCZj4oLUAxSIV34DP5oXC6CB7WlHwzEBcN0yvSwIjzkh9q23N2tRufeN6Ka1jppib/6KcVl0MbX2Lp3elwUxepqNF9TOs9vSOaMcDE+ByJY1l25a62ypbLD3VGfj8Jvt0O3ZB0tt58n9/jsavVNcXU78pA0+P/Yt2L2u9/59G2D8MdY6qJoMr/y29zFuJ6xqkv3LYKdMBSIViivsOIntz8OWx9IXf4DeI6k7Gm2vPFhmH9YDKZK3ZZXt1R94M7YtvswGWD+ovxjGHt7zva5guA/4aATefTbmXvK6Ed20VrdseNXkLAapvVxMVcktiOqZdr3W+dzprMUEPSetzyXRaMxNOJItCPd+dC1dNwY4XAh32qoLPmd64kgItv0DNq6wbulkiMBFt1j32QMfhxX/AredZi3irnYbwB5/lO3MHH8NbH0cGrb3PIcr/FVO7E8tiGHAgo9aKyLcnr74A/QeSe36j2tm28BqKqUjwFoeO52yBHvjAl3xZTYAJi+Ar+3pPbNdzWE2ta77HK9bcZl1hn1dNclaDPFZTt0WxOTYMluD5UKtvX29XhaEMVYgxjgCMf4oWx7DtZiGim+YZTF1NgLOdzRSYhChNtubdnvlrXX2AQqxdOzmYSYQ8dPW+oPQWg8PfAyqZ8DS/9f3e8vGwCV3WBfu2t/Z39qedVC3yf7m3QzJ46+2y7W/6/n+HhbE5ORjlNKACkSqBEvgFGfqignHpO+8/iJAYhaE615yU+L6cjNFo/DXf4flS+zN1uqUJ4gfeZpoQYDt9STi88OUhbHaN+88bZczT7fL6hl2TMRPjokd07TLuqxc8amaYrdlegBPNGKvl6eLKWFQY/sh+9B0LYjysfDZ521GUzoYbi4m936pGG8tiGwWUBwML/wcvj8dbv8nuPsyZ66SupgF4QrEcLAgQq2xWGH8tLX+Ynj3GdvGC34YSynvixmnwsdWwsf/Zl/vXmfLikDs+TJ6Ksw+C9Y/0LtjJn57bSon2sypDM3logIxEBZ9Ci65E+ack75zilgrwrUg2uIsCEguEMZY8/TFn9tSF0/+l91ePMrGDly6LYgUbtqpi6DuDdsLf+fvMO7I2HiJ074AH/ix3ffS7XZb405rNbgDBqsm2fTTTNe8DyUU6nMprgRMbD9Y6wFiFgRA7dz0zigHwyeLqc25X8YdaQdvDfdpZDf8yYr3WV+3nZJfnGwfduXDUCB+fzGs/IJdj5+21l8Uc7e6Y4ZSYfr7YNpi+5Dfs86O9K+cFOvMAMz7gI2h1W2KbWvabd/j88fSyzPkZlKBGAj+IBxzqXcPfCgESuIsCCeDqT8LYu96WHcXnHyDvVne+JPtzc/7oHUxuT0O92FdMtr7PPFMWWRN3Peet6Iz859i+0qqYOHHYfaZsO05e/7GnbH4A1grAzJfb6ZbIDxcTGBFrGk33HkevOqY5/E/unTipssOlyBqe5xAwPB2M0WjdozKrDPg9JvgyvusC3D80ba4HViLT3xWIDauyF2xu2jU9vB3OIPXIqE4C8IRiuoZqXXEEpk4355723P2c8dXaDh8qV1ufiS2rWlXTBiqMptergIxHIgXiO4YhFMiIZnp+OajgMApN8Z8lZMXwMTjYvWG4t9fmopAOL2fh6+37Tnyot7HTD/Fljs/9G7MgnBx5+fet7H//zVYotHec0G4dM8J0Qzr/gA7XrSZXRATr3RTPdM+wLJZybYvugXCydYazgLRuN1anG7K+Jxz4Mp74bP/sL1rsJ2x8lrbIbr/2uyWlI+naae1Gurftm6m+IrArjU62OSVSfNtfbfW/bGkEJfKCTDlRHgzXiB2xwmEa0FkJvanAjEcCJbEZkhrq7f+xerp9nUyC+LNR6xLqKLWBtDFb39UboDLHXDT3gCIM2tcP5RW2+ym1v3wvn+B6R7ZGO4N/MR/2mDi1JNi+6pn2sBxph6W0Sjcdgo84lTc9QpSg41DvPZH+1mC5Ta5YKhTwyajpArGHWXFaDjg3i/uQ7dpGAuE25EYl3Q+MEvFeFt3yERh65OxUcvZxC2PYSK2QKWXBTHY5JWJ82PrM0/rvf/wpdbCOLQtNkjOtRwqJ2DHH6kFkb/UHAZ7XrPrbiEvN8smUSCiERuk2vNazPwcPQ0+/Ywdr+H24ve+bpcdDfYhlqzoVyLH/jPMORfO/pb3/trDoWysdWlVTbaTobj4fNa1EV8uYCBsfRx+fQF8fybce1Vvd8KOF6Fuow0IQu8YRPVMQODRL9kf8cn/B664B5Z8d3DtSZWpi2zF0eEwJ4R7v7gjxodzqmud05FwBy8mo2I8YGz2WajZumKyTfy9WLfJjjWKD1LD4JNXJjkCMWqat6V79CW2M/TgJ23nras1Zrn7g/b6qEDkMbPPsn77Q9usBVFWY83WYHnP0hdrfg3/PdFmfEBMIMBaDsUV1gdae0TsRxRfZiMVTr/JVohMNpBMJGb+n3Jj72Dv+KOsBdG4E/58ffKAddMeeO+FWFbIq3+Auy6x12DOOTZg+bev9XzP6/dZS8kl0cVUOxfO+287KZC/CI78EMz6Jzj64lQ++eCZepJ1ldQ5PeItj8NzP87s/0xG+0FrLRaV26yyOg9rrmOYDDyr22Q7N/0NOnUTJRZ/1pam8RpdnGnqt9qHtC9o7+8eFoSzHKxAVE6wncS553rvr54OH77NpqD/1BWTuDT1DI6FyHq5b8WD2WfZ5dtP2WCcW1CutDrWI/zHzfDYN2D6qbYHUXtE8ilPZ58NL//Kuq3iy2yki+OusJbOgmt67xt/NKz9Lfz1q7aI2IRjYPFneh7TdhCWnwcN79ne1+QF9uafdYYNVAaKbVbWMz+0glE9w/bYNv4ZjvqwHQi4d72322jx/7HxE/Gn/3MnY5rjZtux2n4vf7nRCuT8q9IzZ8hAaD8U+9zT32eF1phY4LP1ANx6is22ueLu7LYtkX0b+3cvQayw5HFX2J785pVw+pesezVb1L9tH+LRiBW2eIEIFNlOWPxc9gPlk0/0PQL7yItg6Q9t52fayTB3SWxf1aSMBe/VggDW7WjgQEsO/JouY+dad80LP7cPGdcP6c4THWqDZ//HFvD66MOw7Gm47i/J56M47CwbUHvvHwO3IFLhiKXwsUe8b2jXxeVWmFz7u1hGVd1mOxnKvVfZ4OkFP7Kpw+EO2wu/7Pcxi+SEj9ng7+rb7fwbq75ur8WxH4HF19ueXOKcFmCvyWlfgFNvTO9n7ovR021btq+G1/8IjTsA0zPzJFu0H4q5J2ecauNE7gyIxsCKz9ny2W+uzF5ZFC/CIesGTKWm2YJr7L0ybh4cf5UNGP9oHrzwi8y306V+q009HzfPWhBt9TEre8oia6UOZX6Y0tH9p14v+pS1JE64tmexyarJGRsHUfAWRENbiCvveJH3zR7LHR89AUnHJEADRcSmj756lzUdT3EebqXV9se97g/2Bjj1X2PzD/TF9FNsZtTWJ+z7MjihSC/if/Dzr7Jt37XWWgt//qwVA18ALvwZzL8y+XlGTbY54C/+3P4h1nqYfab9Yc49L/mkK9lGxA4oXH+/zS4bf4x1OW3+Cyz8WHbbkigQYPPra+fCP35qkxtO+gysvs269c748sDO/+aj9nPOPtv2YhMnkEqVjQ/b8iTjU7AgqmfAiZ+w60dcANe/ZDsMq75uB3dOXWQtzSf/y3a22g7C7rVWVNIxb0s4ZO/fYy61v6sND1iROs+JbZ31tb7fn2nO+284//sZOXXBC8TosiL+7Zy5/Ncjm/jt89s4ZspoDqutYFRZmoq5pcrcJVYglnw35jpZ+HE7fP/RL1nXjev7749gqT321busO2qO9W3uamhnd0M7k0eXMml0H+bsUCittn7l0mpY8j1bsvjXS6xJPnUxfPhWOwgqlVnnzvmO/dxVk+0DOL48yHARB5elP7Ti/updcPY3Ydsz8OJtPSdryiThkA1gth+K+afHzLIDr7Y9azOAHv+WFdnzvmvdJK/eBe+7oXewPxl718P9H7OZPBsetBbezNPh/d+OBVr74rV7rSU8aYF1F049KTat6ECoPRwu+RXcdio8+Am4+k92vvG96yHaZd2WgWK490prbQ/lXjn4jpM9FLUuJvfanv1NmwQxHEhX4UkPJNNTPWeThQsXmjVrBj5/bTRquOpXq3nhHTtI7YTp1dz36ZPx+7JoTRhjzdjEuMLffwBP/ZftcS/4aOrne/lOeOTf4ITr4Jzv0CrlnPHDp9nf3IkI3HrVApYcnSHLYverdkxCzWzbjl1rbZzh+KvTN4I5i3R0RRCB4sAABkhuXw3Lz7UZKCffYGMTRWX2e3737/D8LXb07KXLY+VMBosxdg7qrU/Y1ydcCxf8j11/8FO2x4+xHYWP/MG6JzaugPuusT3iWWfC4efbuETt4b0fOOvugdfvhT2v2+OXPW3Hwmxeaee/bqu36c9TTrQj1qtnWLdb1SQ7jqHtoK2E/Orv7XVo2G7HNnzyiaHFEXaugbsutuNeTBQu+52NY/kC1g3066VWhK64e3Aivec1W8bGnWf6k09Yi6W9d1wvHIligKB/5HntReQVY8xCz325EAgRWQL8FPADvzLGfC9hfzHwO+AEoB74iDFmW3/nHaxAABxo6eQvr+2msT3Mjx9/ixvOPIw54yuYXVvB0ZN73lwHW0OEo1HGVZbQ1NHFW3ubqWvu5Nan32ZPYweXnziVjXuaaGrv4jsXHc2Rk6o8/2coHGX1u/Ws39XI/Cmjed9htsRxXVMH97+ykysXTaO6LGh7e+PmDczHGY1a/7NTDfPmJ7bwo8fe4vuXHMPdq7fzzv5WVvzLqcwcm2LvMU8wxtDQ1oVPhLfqmvnjyzvoDEcJ+IQ9je1MqCrhyElVFAf8+HzC9vpW7l69nYDfx8ULJvPZM2YzrtJW4G3pDBOJGkaVevTgolFY9TUrkO58xWMPtw/Muo32AVlcCQ07bBB/9lnWwhrMeI3Vv7RWZvVMO4Dx9Jts6QqwD/EV/2LjMouW9XRRbnsONv2vjZU07rDbysfZ42aebi2FV++ybsKxc614/NOXe2brtDfAP35ixWnfBvugdvEF7ecMtdi/930OzvpG9+RY0WAFvqF2whq2w1/+1Vqa5yQMonv9fvjzZ6wlNW2xHWDZ2QwnfRoOe3/suH0b7Zzl/iL7OcUHmFiM48SPw6H3rJXo+P7vfWk7neEo5x89gRv/uI7n364n4BO+vOQIPnX6rO5TG2N4ZssBjp08iuryNM1BkmaGlUCIiB94CzgH2Am8DFxhjNkYd8z/AY41xnxGRC4HPmyM+YjnCeMYikC4GGP47F1r+esbNn/cJ3D14unUlBdTURLAGMNPH99CVzTKpSdM4S+v76GhzdbhmVJdysyx5Ty75QC1lban3NTexcULprB41himVJcyvqqEUDjKitd2c/fq7dQ1dzqfGT59+mwmVBVzy1NbOdAS4ogJldxy5QIApo4ppTjgpy1ky0pHovZBd6gtRNTAjJoySoK2h1sc8NHUEe62FrbXt3HD3Ws5bU4tt11zAjsOtvGBnz1Hc0cXR00axaKZtm2hcJS54yuZO6GSiqIAnZEIz7x1gHtf2s64qmJmji1nb2MnFcV+xlbYzxc14PfB2Ipi5k6o5JjJo7p7UZGoYXeDfRiMryqhKJD+3pUxhs5wlLqmTiLGUFNRRGVxgKiB3Q3trNvRQFsoTG1lMXev3s4zWw4QCsceYlUlAWoqigmFo0wYVcKOg23d34n7/X/g2EkY4NH1eygJ+rl68XRm1JTxw1Vv0hmO8oVz5rJoZg0+H7R0hKksCVJTUUR1WRH+tgPW3VT/ts3Uaj9kLcFjLrPxmIevt2mb0bB9QFVOtD14X9BZ+u2DraPJLv0Bm+oZdP7aG6w/fM55NF74a7b99aeYOedx7LH2vmkNhWnpDNPaGaa6rIiaCg8LzhibCLD3desGevuJ2D7x2ZjYWV/vv8RMOGTbcug967M/tM26vaJRzOLPsqd0Dm/vb6GmvJiV6/dw53PvcuVJ07jpvMO77133O+2KGLYfbOOldw/SFYni8wlN7V1UlQaZPbac+dNGU1Zkf49u3DAaNb0F591n4C//5kxRW4oJd0LTbnaMP4tJ/kYCHQdtO4Pl9vPFB3tLRsO1K2irOYpdh9qZMbYcY+BnT27hZ0/agXMBn+DzCZ84dSZv7W3mic11XLJgChfNn8TMseX8/Kmt3PvyDiaPLuW7Fx/DpNElvPjOQTbvbaKmvJhxVcWMqyxhXKVdH1tR3KcVUt/SySvvHaK0yP4GayuLGVNWNCShHW4CcTLwH8aY85zXXwUwxnw37pi/Oce8ICIBYC9Qa/ppbDoEAqCxvYtHXt/DvImV3LdmB/e8tKPH/sWzxlBVEmTVxn0smjGGZafPoqIkwIJp1RQFfOxt7KCmooiGti7+8y8beXJzHS2dvecLOG3OWD568gyOmzqK767czEOv2uHyc8dXcN37ZvLt/32DTudhVhTwUVkcoL61/6qhAZ8Qjva8VBXFAf58/SkcNs76/rfWNbPitT289G49r25v6P4/Xhw2roLWzjD7mjoYX1VCS0eYZo/PA/aBGvD5ELEC4bZDxIpIZUkAjC1IbYxxlhCN+2p9IoiAAOKsR6KGrnCUUMQQjkbpCkfpihhCkd7tDvoFY+h1DUaXBfnQ/MlMHWN76aNKgyw9ZgJlRbFetTGGpg5rGUSihqKAr9tCePdAK99dgFojQwAACz1JREFUuYknNtcRiRqOmlTF6LIg/9han/RajCkvpqo0gNfP121diWnnmMgm5kfWUxM9SIAwARMhSBgfEdqllFZKaZdS/EQoNiGKCVFkOumUYt70HcbK4DlsbZTY/eL3eV6bimL7WbsiUSJRg08Ev8/++QT8PmGSHOBwtuMX2BCYRxOVGGOIGjA4S2O6v7fE1wbv7QlfBwumjWbt9gbKivxUlxXRFYnSHorQ1hUhknhwAgGfUBTw0dEVYUx5MeFo1LEMrSuwKOCjOOCjyPnzOSLS1d7MDe2/5FT/Bt5jIo3+arZFx3Of/4O0+SsZ7W9z7l+hgyK6CLLjYBvhqKEk6LOVXiJRPrJwKmceMY57XtrO586ewwnTq4lEDd9duYnfvfhej07I1Yun8dTm/exqiI09qSwOeP6GRGBMWVG3YNrrFruW9a2hXgV6i/w+ZtWW89cbB+eqHG4CcSmwxBjzSef1NcBJxpgb4o7Z4Byz03n9tnPMAY/zLQOWAUybNu2E9957L+1tbu0MUxTw0dDWxb6mDo6cWIXPJ+w42MaU6tJ+M5+6IlG21rWwt6mDfY0ddEWinD1vfI9AsTGGHQfbCfiF8VUl+H3Cpj1NvLq9gZKgj017mmjuCDN1TFn3j3l0me2lArxX30rYeai1dIYZXRpkwqgSjLG996MmV1FV4h3M6gxHaO2M4PcJb+xuZHt9G62hCEUBHzNryjnlMJupEokaAk7vJhSO4hP7MA9Fouxv7mT9rkY27WkiHLU3tV+EKdVlBHzC7sZ29jR00NIZtg9/EUcAHEEAqwiOeESdB4srJAGfEPD7CPp9FPmFoN9HwFkvCvgYV1VCwCfUt4Q40NrZ/b+PmTyKqtIA79W3sWB6dfcDcigcag2xaW8TJ84YQ8AnvLqjgbqmDiJRqCgJ0NzRxYHmTupbQxxo6aSpPYynQpB0s92X5L5K9p6aiiIuPG4SOw61s2FXI6VBP5UlAcqLA5QV+TnQEmLHwTZ8IgT9VhSizsM7HLHfWSRqiBhD1LmXIlHbQ/dJ3HfliLYv7ruzx/TcjrP0CQhCbWUxc8ZXUN8SYnJ1KQumVfPslv08samOpo4ugj4fpUV+ypy/2spiTppZQ1VpkHA0SlVJkIa2LjbvbeKldw/SGY5SEvRR3xIi6PcxpryIqGNRdnZFCEWidHZF6YxEu9XY5xPOPXI808aUcf8rO+gKG4qDProihnAkSjhq6HLiCe61njamjFm1Fbyxu5Gg38cJ06s5Z974pL329lCEl7cdZG9jBxNGlXD63Foa27t48Z162kJhjpw4irnjKwhHDQdaOqlr6mR/cyd1zZ3UNXdQ19xJR1ek+9rFX/OJVSWcPLuGSNRwoCXE/uYO9jR1EI0avnbB4KZBzmuBiCddFoSiKEqh0JdA5CLkvguIn85sirPN8xjHxTQKG6xWFEVRskQuBOJlYI6IzBSRIuByYEXCMSuAa531S4En+4s/KIqiKOkl6wPljDFhEbkB+Bs2zXW5MeYNEfkOsMYYswK4E/i9iGwFDmJFRFEURckiORlJbYxZCaxM2PbNuPUO4J+z3S5FURQlxsgb9qcoiqJkBRUIRVEUxRMVCEVRFMUTFQhFURTFk7yq5ioi+4HBDqUeC/Q5EC9HaLsGznBtm7ZrYGi7Bs5g2jbdGONZVjevBGIoiMiaZKMJc4m2a+AM17ZpuwaGtmvgpLtt6mJSFEVRPFGBUBRFUTxRgYhxe64bkARt18AZrm3Tdg0MbdfASWvbNAahKIqieKIWhKIoiuKJCoSiKIriScELhIgsEZE3RWSriHwlh+2YKiJPichGEXlDRD7vbP8PEdklIuucv6U5at82EVnvtGGNs22MiDwmIlucZXWW23R43HVZJyJNInJjLq6ZiCwXkTpnsit3m+f1EcvNzj33uogsyEHbfiAim53//5CIjHa2zxCR9rhrd1uW25X0uxORrzrX7E0ROS/L7fpjXJu2icg6Z3s2r1eyZ0Tm7jM7Z2xh/mHLjb8NzAKKgNeAI3PUlonAAme9EngLOBL4D+CLw+BabQPGJmz7f8BXnPWvAN/P8Xe5F5iei2sGnA4sADb0d32ApcCj2BktFwOrc9C2c4GAs/79uLbNiD8uB+3y/O6c38JrQDEw0/nd+rPVroT9/wN8MwfXK9kzImP3WaFbEIuArcaYd4wxIeBe4KJcNMQYs8cYs9ZZbwY2AZNz0ZYBcBHwW2f9t8CHctiWs4G3jTHpn5Q8BYwxz2DnLokn2fW5CPidsbwIjBaRidlsmzFmlTEm7Lx8ETuzY1ZJcs2ScRFwrzGm0xjzLrAV+/vNartERIDLgHsy8b/7oo9nRMbus0IXiMnAjrjXOxkGD2URmQEcD6x2Nt3gmIjLs+3GicMAq0TkFRFZ5mwbb4zZ46zvBcbnpmmAnVQq/kc7HK5Zsusz3O67j2N7mi4zReRVEfm7iJyWg/Z4fXfD5ZqdBuwzxmyJ25b165XwjMjYfVboAjHsEJEK4EHgRmNME3ArMBuYD+zBmre54FRjzALgfOB6ETk9fqexNm1OcqbFTl17IXC/s2m4XLNucnl9+kJEvgaEgT84m/YA04wxxwP/BtwtIlVZbNKw++4SuIKeHZGsXy+PZ0Q36b7PCl0gdgFT415PcbblBBEJYr/4Pxhj/gRgjNlnjIkYY6LAHWTIrO4PY8wuZ1kHPOS0Y59rsjrLuly0DStaa40x+5w2DotrRvLrMyzuOxG5DvgAcJXzYMFx4dQ7669gff1zs9WmPr67nF8zEQkAFwN/dLdl+3p5PSPI4H1W6ALxMjBHRGY6vdDLgRW5aIjj27wT2GSM+VHc9nif4YeBDYnvzULbykWk0l3HBjg3YK/Vtc5h1wIPZ7ttDj16dcPhmjkkuz4rgI86WSaLgcY4F0FWEJElwJeAC40xbXHba0XE76zPAuYA72SxXcm+uxXA5SJSLCIznXa9lK12Obwf2GyM2eluyOb1SvaMIJP3WTai78P5Dxvpfwur/F/LYTtOxZqGrwPrnL+lwO+B9c72FcDEHLRtFjaD5DXgDfc6ATXAE8AW4HFgTA7aVg7UA6PitmX9mmEFag/QhfX1fiLZ9cFmlfzcuefWAwtz0LatWP+0e6/d5hx7ifMdrwPWAh/McruSfnfA15xr9iZwfjbb5Wz/DfCZhGOzeb2SPSMydp9pqQ1FURTFk0J3MSmKoihJUIFQFEVRPFGBUBRFUTxRgVAURVE8UYFQFEVRPFGBUJRhgIicISJ/yXU7FCUeFQhFURTFExUIRRkAInK1iLzk1P7/pYj4RaRFRH7s1Oh/QkRqnWPni8iLEptzwa3Tf5iIPC4ir4nIWhGZ7Zy+QkQeEDtPwx+ckbOKkjNUIBQlRURkHvAR4BRjzHwgAlyFHc29xhhzFPB34FvOW34HfNkYcyx2JKu7/Q/Az40xxwHvw47aBVud80Zsjf9ZwCkZ/1CK0geBXDdAUUYQZwMnAC87nftSbGG0KLECbncBfxKRUcBoY8zfne2/Be53alpNNsY8BGCM6QBwzveScer8iJ2xbAbwXOY/lqJ4owKhKKkjwG+NMV/tsVHkGwnHDbZ+TWfcegT9fSo5Rl1MipI6TwCXisg46J4LeDr2d3Spc8yVwHPGmEbgUNwEMtcAfzd2JrCdIvIh5xzFIlKW1U+hKCmiPRRFSRFjzEYR+Tp2Zj0fttrn9UArsMjZV4eNU4AtvXybIwDvAB9ztl8D/FJEvuOc45+z+DEUJWW0mquiDBERaTHGVOS6HYqSbtTFpCiKoniiFoSiKIriiVoQiqIoiicqEIqiKIonKhCKoiiKJyoQiqIoiicqEIqiKIon/x/HmHzeimo0vwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cdSIVOY8hyBu",
        "outputId": "8953ad9b-27e5-4f1d-c2cc-c4500412abe8"
      },
      "source": [
        "# evaluate the network\n",
        "print(\"[INFO] evaluating network...\")\n",
        "predictions = model.predict(testX, batch_size=32)\n",
        "print(classification_report(testY.argmax(axis=1),\n",
        "\tpredictions.argmax(axis=1), target_names=lb.classes_))"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO] evaluating network...\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      GMB_02       1.00      0.78      0.88        27\n",
            "      GMB_04       1.00      0.89      0.94        28\n",
            "      GMB_07       0.89      1.00      0.94        32\n",
            "      GMB_08       1.00      0.94      0.97        32\n",
            "      GMB_10       0.77      0.96      0.86        28\n",
            "\n",
            "    accuracy                           0.92       147\n",
            "   macro avg       0.93      0.91      0.92       147\n",
            "weighted avg       0.93      0.92      0.92       147\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5Ojqs3qhz4R"
      },
      "source": [
        "model.save('nnmodel_daun')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}